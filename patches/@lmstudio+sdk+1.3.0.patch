diff --git a/node_modules/@lmstudio/sdk/dist/index.cjs b/node_modules/@lmstudio/sdk/dist/index.cjs
index 7de5ae1..c174fad 100644
--- a/node_modules/@lmstudio/sdk/dist/index.cjs
+++ b/node_modules/@lmstudio/sdk/dist/index.cjs
@@ -1,11 +1,14 @@
 'use strict';
 
 var zod = require('zod');
-var process$1 = require('process');
 var chalk = require('chalk');
 var lmsIsomorphic = require('@lmstudio/lms-isomorphic');
 var zodToJsonSchema = require('zod-to-json-schema');
 
+if (typeof process == 'undefined') {
+    process = {}
+}
+
 function isSignalLike(value) {
     return (typeof value === "object" &&
         value !== null &&
@@ -2042,6 +2045,7 @@ class Cleaner {
     }
 }
 
+// import process from "process";
 // Error stack manipulation related functions
 function getCurrentStack(goAbove = 0) {
     const stack = new Error().stack;
@@ -2052,7 +2056,7 @@ function getCurrentStack(goAbove = 0) {
     return lines.slice(2 + goAbove).join("\n");
 }
 function changeErrorStackInPlace(error, newStack) {
-    if (process$1.env.LMS_KEEP_INTERNAL_STACK) {
+    if (process.env.LMS_KEEP_INTERNAL_STACK) {
         return;
     }
     const stackContent = error.stack ?? "";
@@ -2061,6 +2065,17 @@ function changeErrorStackInPlace(error, newStack) {
         newStack).trimEnd();
 }
 
+class IdGiver {
+    constructor(firstId = 0) {
+        this.nextId = firstId;
+    }
+    next() {
+        const id = this.nextId;
+        this.nextId++;
+        return id;
+    }
+}
+
 function getDefaultExportFromCjs (x) {
 	return x && x.__esModule && Object.prototype.hasOwnProperty.call(x, 'default') ? x['default'] : x;
 }
@@ -4421,7 +4436,7 @@ function makeTitledPrettyError(title, content, stack) {
     return makePrettyError(chalk.redBright(title) + "\n\n" + content, stack);
 }
 function makePrettyError(content, stack) {
-    if (process$1.browser || process$1.env.LMS_NO_FANCY_ERRORS || lmsIsomorphic.terminalSize().columns < 80) {
+    if (process.browser || process.env.LMS_NO_FANCY_ERRORS || lmsIsomorphic.terminalSize().columns < 80) {
         const error = new Error(content);
         if (stack === undefined) {
             changeErrorStackInPlace(error, "");
@@ -5087,6 +5102,10 @@ const artifactManifestBaseSchema = zod.z.object({
     dependencies: zod.z.array(artifactDependencySchema).optional(),
     tags: zod.z.array(zod.z.string()).optional(),
 });
+const artifactIdentifierRegex = /^[a-z0-9]+(?:-[a-z0-9]+)*\/[a-z0-9]+(?:[-.][a-z0-9]+)*$/;
+const artifactIdentifierSchema = zod.z.string().regex(artifactIdentifierRegex, {
+    message: "Invalid artifact identifier format. Expected 'owner/name'.",
+});
 
 const modelManifestSchema = zod.z.object({
     type: zod.z.literal("model"),
@@ -5633,6 +5652,7 @@ const llmToolParametersSchema = zod.z.discriminatedUnion("type", [
         properties: zod.z.record(jsonSerializableSchema),
         required: zod.z.array(zod.z.string()).optional(),
         additionalProperties: zod.z.boolean().optional(),
+        $defs: zod.z.record(jsonSerializableSchema).optional(),
     }),
     // add more parameter types here
     // ...
@@ -6115,6 +6135,17 @@ zod.z.discriminatedUnion("type", [
     processingUpdateDebugInfoBlockCreateSchema,
 ]);
 
+const tokenSourceIdentifierSchema = zod.z.discriminatedUnion("type", [
+    zod.z.object({
+        type: zod.z.literal("model"),
+        identifier: zod.z.string(),
+    }),
+    zod.z.object({
+        type: zod.z.literal("generator"),
+        pluginIdentifier: zod.z.string(),
+    }),
+]);
+
 const modelInfoSchema = zod.z.discriminatedUnion("type", [
     llmInfoSchema,
     embeddingModelInfoSchema,
@@ -6124,6 +6155,29 @@ const modelInstanceInfoSchema = zod.z.discriminatedUnion("type", [
     embeddingModelInstanceInfoSchema,
 ]);
 
+const pluginConfigSpecifierSchema = zod.z.discriminatedUnion("type", [
+    zod.z.object({
+        type: zod.z.literal("direct"),
+        config: kvConfigSchema,
+        workingDirectoryPath: zod.z.string().optional(),
+    }),
+    zod.z.object({
+        type: zod.z.literal("predictionProcess"),
+        pci: zod.z.string(),
+        token: zod.z.string(),
+    }),
+]);
+
+const remotePluginInfoSchema = zod.z.object({
+    identifier: zod.z.string(),
+    isDev: zod.z.boolean(),
+    isTrusted: zod.z.boolean(),
+    hasPromptPreprocessor: zod.z.boolean(),
+    hasPredictionLoopHandler: zod.z.boolean(),
+    hasToolsProvider: zod.z.boolean(),
+    hasGenerator: zod.z.boolean(),
+});
+
 const artifactDownloadPlanModelInfoSchema = zod.z.object({
     displayName: zod.z.string(),
     sizeBytes: zod.z.number(),
@@ -9319,98 +9373,6 @@ function maybeFalseNumberToCheckboxNumeric(maybeFalseNumber, valueWhenUnchecked)
     return { checked: true, value: maybeFalseNumber };
 }
 
-function kvConfigToLLMPredictionConfig(config) {
-    const result = {};
-    const parsed = globalConfigSchematics.parsePartial(config);
-    const maxPredictedTokens = parsed.get("llm.prediction.maxPredictedTokens");
-    if (maxPredictedTokens !== undefined) {
-        result.maxTokens = maxPredictedTokens.checked ? maxPredictedTokens.value : false;
-    }
-    const temperature = parsed.get("llm.prediction.temperature");
-    if (temperature !== undefined) {
-        result.temperature = temperature;
-    }
-    const stopStrings = parsed.get("llm.prediction.stopStrings");
-    if (stopStrings !== undefined) {
-        result.stopStrings = stopStrings;
-    }
-    const toolCallStopStrings = parsed.get("llm.prediction.toolCallStopStrings");
-    if (toolCallStopStrings !== undefined) {
-        result.toolCallStopStrings = toolCallStopStrings;
-    }
-    const contextOverflowPolicy = parsed.get("llm.prediction.contextOverflowPolicy");
-    if (contextOverflowPolicy !== undefined) {
-        result.contextOverflowPolicy = contextOverflowPolicy;
-    }
-    const structured = parsed.get("llm.prediction.structured");
-    if (structured !== undefined) {
-        result.structured = structured;
-    }
-    const tools = parsed.get("llm.prediction.tools");
-    if (tools !== undefined) {
-        result.rawTools = tools;
-    }
-    const topKSampling = parsed.get("llm.prediction.topKSampling");
-    if (topKSampling !== undefined) {
-        result.topKSampling = topKSampling;
-    }
-    const repeatPenalty = parsed.get("llm.prediction.repeatPenalty");
-    if (repeatPenalty !== undefined) {
-        result.repeatPenalty = repeatPenalty.checked ? repeatPenalty.value : false;
-    }
-    const minPSampling = parsed.get("llm.prediction.minPSampling");
-    if (minPSampling !== undefined) {
-        result.minPSampling = minPSampling.checked ? minPSampling.value : false;
-    }
-    const topPSampling = parsed.get("llm.prediction.topPSampling");
-    if (topPSampling !== undefined) {
-        result.topPSampling = topPSampling.checked ? topPSampling.value : false;
-    }
-    const xtcProbability = parsed.get("llm.prediction.llama.xtcProbability");
-    if (xtcProbability !== undefined) {
-        result.xtcProbability = xtcProbability.checked ? xtcProbability.value : false;
-    }
-    const xtcThreshold = parsed.get("llm.prediction.llama.xtcThreshold");
-    if (xtcThreshold !== undefined) {
-        result.xtcThreshold = xtcThreshold.checked ? xtcThreshold.value : false;
-    }
-    const logProbs = parsed.get("llm.prediction.logProbs");
-    if (logProbs !== undefined) {
-        result.logProbs = logProbs.checked ? logProbs.value : false;
-    }
-    const cpuThreads = parsed.get("llm.prediction.llama.cpuThreads");
-    if (cpuThreads !== undefined) {
-        result.cpuThreads = cpuThreads;
-    }
-    const promptTemplate = parsed.get("llm.prediction.promptTemplate");
-    if (promptTemplate !== undefined) {
-        result.promptTemplate = promptTemplate;
-    }
-    const speculativeDecodingDraftModel = parsed.get("llm.prediction.speculativeDecoding.draftModel");
-    if (speculativeDecodingDraftModel !== undefined) {
-        result.draftModel = speculativeDecodingDraftModel;
-    }
-    const speculativeDecodingDraftTokensExact = parsed.get("llm.prediction.speculativeDecoding.numDraftTokensExact");
-    if (speculativeDecodingDraftTokensExact !== undefined) {
-        result.speculativeDecodingNumDraftTokensExact = speculativeDecodingDraftTokensExact;
-    }
-    const speculativeDecodingMinContinueDraftingProbability = parsed.get("llm.prediction.speculativeDecoding.minContinueDraftingProbability");
-    if (speculativeDecodingMinContinueDraftingProbability !== undefined) {
-        result.speculativeDecodingMinContinueDraftingProbability =
-            speculativeDecodingMinContinueDraftingProbability;
-    }
-    const speculativeDecodingMinDraftLengthToConsider = parsed.get("llm.prediction.speculativeDecoding.minDraftLengthToConsider");
-    if (speculativeDecodingMinDraftLengthToConsider !== undefined) {
-        result.speculativeDecodingMinDraftLengthToConsider =
-            speculativeDecodingMinDraftLengthToConsider;
-    }
-    const reasoningParsing = parsed.get("llm.prediction.reasoning.parsing");
-    if (reasoningParsing !== undefined) {
-        result.reasoningParsing = reasoningParsing;
-    }
-    result.raw = config;
-    return result;
-}
 function llmPredictionConfigToKVConfig(config) {
     const top = llmPredictionConfigSchematics.buildPartialConfig({
         "temperature": config.temperature,
@@ -11321,14 +11283,34 @@ class SimpleToolCallContext {
 const functionToolSchema = toolBaseSchema.extend({
     type: zod.z.literal("function"),
     parametersSchema: zodSchemaSchema,
+    checkParameters: zod.z.function(),
     implementation: zod.z.function(),
 });
 const rawFunctionToolSchema = toolBaseSchema.extend({
     type: zod.z.literal("rawFunction"),
     parametersSchema: zodSchemaSchema,
+    checkParameters: zod.z.function(),
+    implementation: zod.z.function(),
+});
+const unimplementedRawFunctionToolSchema = toolBaseSchema.extend({
+    type: zod.z.literal("unimplementedRawFunction"),
+    parametersJsonSchema: zodSchemaSchema,
+    checkParameters: zod.z.function(),
+    implementation: zod.z.function(),
+});
+const remoteToolSchema = toolBaseSchema.extend({
+    type: zod.z.literal("remoteTool"),
+    pluginIdentifier: zod.z.string(),
+    parametersJsonSchema: zodSchemaSchema,
+    checkParameters: zod.z.function(),
     implementation: zod.z.function(),
 });
-zod.z.discriminatedUnion("type", [functionToolSchema, rawFunctionToolSchema]);
+zod.z.discriminatedUnion("type", [
+    functionToolSchema,
+    rawFunctionToolSchema,
+    unimplementedRawFunctionToolSchema,
+    remoteToolSchema,
+]);
 /**
  * A function that can be used to create a function `Tool` given a function definition and its
  * implementation.
@@ -11397,6 +11379,63 @@ function rawFunctionTool({ name, description, parametersJsonSchema, implementati
         implementation,
     };
 }
+class UnimplementedToolError extends Error {
+    constructor(toolName) {
+        super(`Tool "${toolName}" is not implemented.`);
+    }
+}
+/**
+ * A function that can be used to create a raw function `Tool` that is not implemented yet. When
+ * using `.act`, upon encountering an unimplemented tool, the `.act` will stop gracefully.
+ *
+ * @public
+ * @experimental Not stable, will likely change in the future.
+ */
+function unimplementedRawFunctionTool({ name, description, parametersJsonSchema, }) {
+    const jsonSchemaValidator = new libExports.Validator();
+    return {
+        name,
+        description,
+        type: "unimplementedRawFunction",
+        parametersJsonSchema,
+        checkParameters(params) {
+            const validationResult = jsonSchemaValidator.validate(params, parametersJsonSchema);
+            if (validationResult.errors.length > 0) {
+                throw new Error(text `
+          Failed to parse arguments for tool "${name}":
+          ${jsonSchemaValidationErrorToAIReadableText("params", validationResult.errors)}
+        `);
+            }
+        },
+        implementation: () => {
+            throw new UnimplementedToolError(name);
+        },
+    };
+}
+/**
+ * Creates a tool that represents a remote tool exposed by an LMStudio plugin. This function is not
+ * exposed and is used internally by the plugins namespace.
+ */
+function internalCreateRemoteTool({ name, description, pluginIdentifier, parametersJsonSchema, implementation, }) {
+    return {
+        name,
+        description,
+        type: "remoteTool",
+        pluginIdentifier,
+        parametersJsonSchema,
+        checkParameters: params => {
+            const jsonSchemaValidator = new libExports.Validator();
+            const validationResult = jsonSchemaValidator.validate(params, parametersJsonSchema);
+            if (validationResult.errors.length > 0) {
+                throw new Error(text `
+          Failed to parse arguments for tool "${name}":
+          ${jsonSchemaValidationErrorToAIReadableText("params", validationResult.errors)}
+        `);
+            }
+        },
+        implementation,
+    };
+}
 function functionToolToLLMTool(tool) {
     return {
         type: "function",
@@ -11417,6 +11456,16 @@ function rawFunctionToolToLLMTool(tool) {
         },
     };
 }
+function remoteToolToLLMTool(tool) {
+    return {
+        type: "function",
+        function: {
+            name: tool.name,
+            description: tool.description,
+            parameters: tool.parametersJsonSchema,
+        },
+    };
+}
 /**
  * Convert a `Tool` to a internal `LLMTool`.
  */
@@ -11426,7 +11475,10 @@ function toolToLLMTool(tool) {
         case "function":
             return functionToolToLLMTool(tool);
         case "rawFunction":
+        case "unimplementedRawFunction":
             return rawFunctionToolToLLMTool(tool);
+        case "remoteTool":
+            return remoteToolToLLMTool(tool);
         default: {
             const exhaustiveCheck = type;
             throw new Error(`Unhandled type: ${exhaustiveCheck}`);
@@ -13995,6 +14047,10 @@ function createLlmBackendInterface() {
             }),
             zod.z.object({
                 type: zod.z.literal("toolCallGenerationStart"),
+                /**
+                 * The LLM specific call id of the tool call.
+                 */
+                toolCallId: zod.z.string().optional(),
             }),
             zod.z.object({
                 type: zod.z.literal("toolCallGenerationNameReceived"),
@@ -14041,13 +14097,134 @@ function createLlmBackendInterface() {
                 type: zod.z.literal("cancel"),
             }),
         ]),
+    })
+        .addRpcEndpoint("applyPromptTemplate", {
+        parameter: zod.z.object({
+            specifier: modelSpecifierSchema,
+            history: chatHistoryDataSchema,
+            predictionConfigStack: kvConfigStackSchema,
+            opts: llmApplyPromptTemplateOptsSchema,
+        }),
+        returns: zod.z.object({
+            formatted: zod.z.string(),
+        }),
+    })
+        .addRpcEndpoint("tokenize", {
+        parameter: zod.z.object({
+            specifier: modelSpecifierSchema,
+            inputString: zod.z.string(),
+        }),
+        returns: zod.z.object({
+            tokens: zod.z.array(zod.z.number()),
+        }),
+    })
+        .addRpcEndpoint("countTokens", {
+        parameter: zod.z.object({
+            specifier: modelSpecifierSchema,
+            inputString: zod.z.string(),
+        }),
+        returns: zod.z.object({
+            tokenCount: zod.z.number().int(),
+        }),
+    })
+        // Starts to eagerly preload a draft model. This is useful when you want a draft model to be
+        // ready for speculative decoding.
+        .addRpcEndpoint("preloadDraftModel", {
+        parameter: zod.z.object({
+            specifier: modelSpecifierSchema,
+            draftModelKey: zod.z.string(),
+        }),
+        returns: zod.z.void(),
+    }));
+}
+
+function createPluginsBackendInterface() {
+    return (new BackendInterface()
+        /**
+         * The following method is called by a client that wants to use plugins that are registered
+         * to LM Studio.
+         */
+        .addChannelEndpoint("startToolUseSession", {
+        creationParameter: zod.z.object({
+            pluginIdentifier: zod.z.string(),
+            pluginConfigSpecifier: pluginConfigSpecifierSchema,
+        }),
+        toClientPacket: zod.z.discriminatedUnion("type", [
+            /**
+             * The session has been started successfully. The client can now use the session. Note,
+             * there are no sessionError message because if a session fails to start, the channel
+             * will error instead.
+             */
+            zod.z.object({
+                type: zod.z.literal("sessionReady"),
+                toolDefinitions: zod.z.array(llmToolSchema),
+            }),
+            /**
+             * A tool call has been completed.
+             */
+            zod.z.object({
+                type: zod.z.literal("toolCallComplete"),
+                callId: zod.z.number(),
+                result: jsonSerializableSchema,
+            }),
+            /**
+             * A tool call has failed.
+             */
+            zod.z.object({
+                type: zod.z.literal("toolCallError"),
+                callId: zod.z.number(),
+                error: serializedLMSExtendedErrorSchema,
+            }),
+            /**
+             * Status update for a tool call.
+             */
+            zod.z.object({
+                type: zod.z.literal("toolCallStatus"),
+                callId: zod.z.number(),
+                statusText: zod.z.string(),
+            }),
+            /**
+             * Warning message for a tool call.
+             */
+            zod.z.object({
+                type: zod.z.literal("toolCallWarn"),
+                callId: zod.z.number(),
+                warnText: zod.z.string(),
+            }),
+        ]),
+        toServerPacket: zod.z.discriminatedUnion("type", [
+            /**
+             * Request to start a tool call. This call can be aborted using the `abortToolCall`
+             * packet. When the tool call is completed, either the `toolCallResult` or `toolCallError`
+             * packet will be sent.
+             */
+            zod.z.object({
+                type: zod.z.literal("callTool"),
+                callId: zod.z.number(),
+                name: zod.z.string(),
+                arguments: jsonSerializableSchema,
+            }),
+            /**
+             * Request to abort a tool call. Upon calling this, no toolCallComplete or toolCallError
+             * packets will be sent for the call. We assume abort is done immediately.
+             */
+            zod.z.object({
+                type: zod.z.literal("abortToolCall"),
+                callId: zod.z.number(),
+            }),
+            /**
+             * Client requests to discard the session. Upon calling this, the channel will be closed.
+             */
+            zod.z.object({
+                type: zod.z.literal("discardSession"),
+            }),
+        ]),
     })
         .addChannelEndpoint("generateWithGenerator", {
         creationParameter: zod.z.object({
             pluginIdentifier: zod.z.string(),
-            pluginConfigStack: kvConfigStackSchema,
+            pluginConfigSpecifier: pluginConfigSpecifierSchema,
             tools: zod.z.array(llmToolSchema),
-            workingDirectoryPath: zod.z.string().nullable(),
             history: chatHistoryDataSchema,
         }),
         toClientPacket: zod.z.discriminatedUnion("type", [
@@ -14061,6 +14238,10 @@ function createLlmBackendInterface() {
             }),
             zod.z.object({
                 type: zod.z.literal("toolCallGenerationStart"),
+                /**
+                 * The LLM specific call id of the tool call.
+                 */
+                toolCallId: zod.z.string().optional(),
             }),
             zod.z.object({
                 type: zod.z.literal("toolCallGenerationNameReceived"),
@@ -14087,48 +14268,6 @@ function createLlmBackendInterface() {
             }),
         ]),
     })
-        .addRpcEndpoint("applyPromptTemplate", {
-        parameter: zod.z.object({
-            specifier: modelSpecifierSchema,
-            history: chatHistoryDataSchema,
-            predictionConfigStack: kvConfigStackSchema,
-            opts: llmApplyPromptTemplateOptsSchema,
-        }),
-        returns: zod.z.object({
-            formatted: zod.z.string(),
-        }),
-    })
-        .addRpcEndpoint("tokenize", {
-        parameter: zod.z.object({
-            specifier: modelSpecifierSchema,
-            inputString: zod.z.string(),
-        }),
-        returns: zod.z.object({
-            tokens: zod.z.array(zod.z.number()),
-        }),
-    })
-        .addRpcEndpoint("countTokens", {
-        parameter: zod.z.object({
-            specifier: modelSpecifierSchema,
-            inputString: zod.z.string(),
-        }),
-        returns: zod.z.object({
-            tokenCount: zod.z.number().int(),
-        }),
-    })
-        // Starts to eagerly preload a draft model. This is useful when you want a draft model to be
-        // ready for speculative decoding.
-        .addRpcEndpoint("preloadDraftModel", {
-        parameter: zod.z.object({
-            specifier: modelSpecifierSchema,
-            draftModelKey: zod.z.string(),
-        }),
-        returns: zod.z.void(),
-    }));
-}
-
-function createPluginsBackendInterface() {
-    return (new BackendInterface()
         /**
          * The following method is called by the controlling client. (e.g. lms-cli)
          */
@@ -14167,6 +14306,10 @@ function createPluginsBackendInterface() {
                 pluginConfig: kvConfigSchema,
                 globalPluginConfig: kvConfigSchema,
                 workingDirectoryPath: zod.z.string().nullable(),
+                /**
+                 * An array of all the plugins that are enabled for this prediction.
+                 */
+                enabledPluginInfos: zod.z.array(remotePluginInfoSchema),
                 /** Processing Context Identifier */
                 pci: zod.z.string(),
                 token: zod.z.string(),
@@ -14203,6 +14346,10 @@ function createPluginsBackendInterface() {
                 pluginConfig: kvConfigSchema,
                 globalPluginConfig: kvConfigSchema,
                 workingDirectoryPath: zod.z.string().nullable(),
+                /**
+                 * An array of all the plugins that are enabled for this prediction.
+                 */
+                enabledPluginInfos: zod.z.array(remotePluginInfoSchema),
                 /** Processing Context Identifier */
                 pci: zod.z.string(),
                 token: zod.z.string(),
@@ -14353,6 +14500,7 @@ function createPluginsBackendInterface() {
             zod.z.object({
                 type: zod.z.literal("toolCallGenerationStarted"),
                 taskId: zod.z.string(),
+                toolCallId: zod.z.string().optional(),
             }),
             zod.z.object({
                 type: zod.z.literal("toolCallGenerationNameReceived"),
@@ -14405,14 +14553,14 @@ function createPluginsBackendInterface() {
         }),
         returns: chatHistoryDataSchema,
     })
-        .addRpcEndpoint("processingGetOrLoadModel", {
+        .addRpcEndpoint("processingGetOrLoadTokenSource", {
         parameter: zod.z.object({
             /** Processing Context Identifier */
             pci: zod.z.string(),
             token: zod.z.string(),
         }),
         returns: zod.z.object({
-            identifier: zod.z.string(),
+            tokenSourceIdentifier: tokenSourceIdentifierSchema,
         }),
     })
         .addRpcEndpoint("processingHasStatus", {
@@ -15827,6 +15975,11 @@ class ActResult {
     }
 }
 
+/**
+ * Each call uses a globally unique call ID that starts somewhere before the half of the
+ * `Number.MAX_SAFE_INTEGER`.
+ */
+const callIdGiver = new IdGiver(Math.floor(Math.random() * (Number.MAX_SAFE_INTEGER / 2 / 10000)) * 10000);
 class NoQueueQueue {
     needsQueueing() {
         return false;
@@ -15958,9 +16111,58 @@ class FIFOQueue {
         this.queue = [];
     }
 }
-const llmActBaseOptsSchema = zod.z.object({
-    onFirstToken: zod.z.function().optional(),
-    onPredictionFragment: zod.z.function().optional(),
+/**
+ * Controller object used to allow/modify/deny a tool call.
+ */
+class GuardToolCallController {
+    /**
+     * Don't construct this object yourself.
+     */
+    constructor(toolCallRequest, tool, resultContainer) {
+        this.toolCallRequest = toolCallRequest;
+        this.tool = tool;
+        this.resultContainer = resultContainer;
+        /**
+         * Allows the tool call to proceed without any modifications.
+         */
+        this.allow = () => {
+            this.assertNoResultYet("allow", getCurrentStack(1));
+            this.resultContainer[0] = { type: "allow" };
+        };
+        /**
+         * Allows the tool call to proceed, but overrides the parameters with the provided ones.
+         */
+        this.allowAndOverrideParameters = (newParameters) => {
+            this.assertNoResultYet("allowAndOverrideParameters", getCurrentStack(1));
+            this.resultContainer[0] = { type: "allowAndOverrideParameters", parameters: newParameters };
+        };
+        /**
+         * Denys the tool call with a specified reason. This will not interrupt the `.act` call. Instead,
+         * the reason you provide will be provided to the model as the tool call result.
+         *
+         * If `reason` is not provided, a generic default reason will be used.
+         *
+         * If you wish to immediately fail the `.act` call, you can throw an error instead.
+         */
+        this.deny = (reason) => {
+            this.assertNoResultYet("deny", getCurrentStack(1));
+            this.resultContainer[0] = { type: "deny", reason };
+        };
+    }
+    assertNoResultYet(calledMethodName, stack) {
+        if (this.resultContainer[0] === null) {
+            return;
+        }
+        // Oh no, the result has already been set! Make an error message.
+        throw makeTitledPrettyError(`Cannot call ${calledMethodName} after a result has been set`, text `
+        This tool call guard has already set a result previously (${this.resultContainer[0].type}). 
+        You cannot set a result more than once.
+      `, stack);
+    }
+}
+const llmActBaseOptsSchema = zod.z.object({
+    onFirstToken: zod.z.function().optional(),
+    onPredictionFragment: zod.z.function().optional(),
     onMessage: zod.z.function().optional(),
     onRoundStart: zod.z.function().optional(),
     onRoundEnd: zod.z.function().optional(),
@@ -15970,8 +16172,10 @@ const llmActBaseOptsSchema = zod.z.object({
     onToolCallRequestNameReceived: zod.z.function().optional(),
     onToolCallRequestArgumentFragmentGenerated: zod.z.function().optional(),
     onToolCallRequestEnd: zod.z.function().optional(),
+    onToolCallRequestFinalized: zod.z.function().optional(),
     onToolCallRequestFailure: zod.z.function().optional(),
     onToolCallRequestDequeued: zod.z.function().optional(),
+    guardToolCall: zod.z.function().optional(),
     handleInvalidToolRequest: zod.z.function().optional(),
     maxPredictionRounds: zod.z.number().int().min(1).optional(),
     signal: zod.z.instanceof(AbortSignal).optional(),
@@ -16011,10 +16215,12 @@ const defaultHandleInvalidToolRequest = (error, request) => {
 async function internalAct(chat, tools, baseOpts, stack, logger, startTime, predictImpl, makePredictionResult) {
     const abortController = new AbortController();
     const mutableChat = Chat.from(chat); // Make a copy
+    let currentCallId = -1;
     /**
-     * Our ID that allows users to match up calls.
+     * A flag that will be set if any unimplemented tool is called. In which case, the loop will
+     * terminate after all the parallel tool calls are resolved.
      */
-    let currentCallId = 0;
+    let hasCalledUnimplementedTool = false;
     if (baseOpts.signal !== undefined) {
         if (baseOpts.signal.aborted) {
             // If the signal is already aborted, we should not continue.
@@ -16133,6 +16339,8 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
         const queue = baseOpts.allowParallelToolExecution
             ? new NoQueueQueue()
             : new FIFOQueue();
+        let receivedEagerToolNameReporting = false;
+        let receivedToolArgumentsStreaming = false;
         predictImpl({
             allowTools,
             history: accessMaybeMutableInternals(mutableChat)._internalGetData(),
@@ -16156,27 +16364,43 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
             handlePromptProcessingProgress: progress => {
                 safeCallCallback(logger, "onPromptProcessingProgress", baseOpts.onPromptProcessingProgress, [predictionsPerformed, progress]);
             },
-            handleToolCallGenerationStart: () => {
-                currentCallId++;
+            handleToolCallGenerationStart: toolCallId => {
+                currentCallId = callIdGiver.next();
+                receivedEagerToolNameReporting = false;
+                receivedToolArgumentsStreaming = false;
                 isGeneratingToolCall = true;
                 safeCallCallback(logger, "onToolCallRequestStart", baseOpts.onToolCallRequestStart, [
                     predictionsPerformed,
                     currentCallId,
+                    { toolCallId: toolCallId },
                 ]);
             },
             handleToolCallGenerationNameReceived: name => {
+                receivedEagerToolNameReporting = true;
                 safeCallCallback(logger, "onToolCallRequestNameReceived", baseOpts.onToolCallRequestNameReceived, [predictionsPerformed, currentCallId, name]);
             },
             handleToolCallGenerationArgumentFragmentGenerated: content => {
+                receivedToolArgumentsStreaming = true;
                 safeCallCallback(logger, "onToolCallRequestArgumentFragmentGenerated", baseOpts.onToolCallRequestArgumentFragmentGenerated, [predictionsPerformed, currentCallId, content]);
             },
             handleToolCallGenerationEnd: (request, rawContent) => {
+                const callId = currentCallId;
                 isGeneratingToolCall = false;
                 const toolCallIndex = nextToolCallIndex;
                 nextToolCallIndex++;
+                if (!receivedEagerToolNameReporting) {
+                    // If eager name reporting not received, report it.
+                    safeCallCallback(logger, "onToolCallRequestNameReceived", baseOpts.onToolCallRequestNameReceived, [predictionsPerformed, callId, request.name]);
+                }
+                if (!receivedToolArgumentsStreaming) {
+                    // If arguments streaming not received, just pretend we have received all the arguments
+                    // as a single JSON
+                    safeCallCallback(logger, "onToolCallRequestArgumentFragmentGenerated", baseOpts.onToolCallRequestArgumentFragmentGenerated, [predictionsPerformed, callId, JSON.stringify(request.arguments ?? {}, null, 2)]);
+                }
+                const pushedRequest = { ...request };
                 // We have now received a tool call request. Now let's see if we can call the tool and
                 // get the result.
-                toolCallRequests.push(request);
+                toolCallRequests.push(pushedRequest);
                 const tool = toolsMap.get(request.name);
                 if (tool === undefined) {
                     // Tool does not exist.
@@ -16184,15 +16408,14 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
                     toolCallPromises.push(internalHandleInvalidToolCallRequest(toolCallRequestError, request, toolCallIndex).catch(finalReject));
                     safeCallCallback(logger, "onToolCallRequestFailure", baseOpts.onToolCallRequestFailure, [
                         predictionsPerformed,
-                        currentCallId,
+                        callId,
                         toolCallRequestError,
                     ]);
                     return;
                 }
-                const parameters = request.arguments ?? {}; // Defaults to empty object
                 // Try check the parameters:
                 try {
-                    tool.checkParameters(parameters); // Defaults to empty object
+                    tool.checkParameters(pushedRequest.arguments); // Defaults to empty object
                 }
                 catch (error) {
                     // Failed to parse the parameters
@@ -16200,16 +16423,16 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
                     toolCallPromises.push(internalHandleInvalidToolCallRequest(toolCallRequestError, request, toolCallIndex).catch(finalReject));
                     safeCallCallback(logger, "onToolCallRequestFailure", baseOpts.onToolCallRequestFailure, [
                         predictionsPerformed,
-                        currentCallId,
+                        callId,
                         toolCallRequestError,
                     ]);
                     return;
                 }
-                const toolCallContext = new SimpleToolCallContext(new SimpleLogger(`Tool(${request.name})`, logger), abortController.signal, currentCallId);
+                const toolCallContext = new SimpleToolCallContext(new SimpleLogger(`Tool(${request.name})`, logger), abortController.signal, callId);
                 const isQueued = queue.needsQueueing();
                 safeCallCallback(logger, "onToolCallRequestEnd", baseOpts.onToolCallRequestEnd, [
                     predictionsPerformed,
-                    currentCallId,
+                    callId,
                     {
                         isQueued,
                         toolCallRequest: request,
@@ -16219,10 +16442,71 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
                 // We have successfully parsed the parameters. Let's call the tool.
                 toolCallPromises.push(queue
                     .runInQueue(async () => {
+                    // Emit the dequeued event if the tool call was queued.
                     if (isQueued) {
-                        safeCallCallback(logger, "onToolCallRequestDequeued", baseOpts.onToolCallRequestDequeued, [predictionsPerformed, currentCallId]);
+                        safeCallCallback(logger, "onToolCallRequestDequeued", baseOpts.onToolCallRequestDequeued, [predictionsPerformed, callId]);
+                    }
+                    // Guard the tool call if have a tool call guard.
+                    if (baseOpts.guardToolCall !== undefined) {
+                        const resultContainer = [null];
+                        await baseOpts.guardToolCall(predictionsPerformed, callId, new GuardToolCallController(request, tool, resultContainer));
+                        if (resultContainer[0] === null) {
+                            // The guard did not return anything, thus we will report this error.
+                            throw makeTitledPrettyError("Tool call guard did not allow or deny the tool call.", text `
+                      The \`guardToolCall\` handler must call one of the methods on the controller
+                      to allow or deny the tool call.
+                    `, stack);
+                        }
+                        const guardResult = resultContainer[0];
+                        const guardResultType = guardResult.type;
+                        switch (guardResultType) {
+                            case "allow": {
+                                // 1. The guard allowed the tool call without overriding the parameters. In this
+                                //    case, we will use the original parameters.
+                                break;
+                            }
+                            case "allowAndOverrideParameters": {
+                                // 2. The guard allowed the tool call and provided new parameters. In this case,
+                                //    we will use the new parameters. This will update the request in-place.
+                                pushedRequest.arguments = guardResult.parameters;
+                                break;
+                            }
+                            case "deny": {
+                                // 3. The guard denied the tool call. In this case, we will early return and not
+                                //    call the tool.
+                                toolCallResults.push({
+                                    index: toolCallIndex,
+                                    data: {
+                                        type: "toolCallResult",
+                                        toolCallId: request.id,
+                                        content: JSON.stringify({
+                                            error: guardResult.reason,
+                                        }),
+                                    },
+                                });
+                                return;
+                            }
+                        }
+                    }
+                    // Now we need to call RequestFinalized
+                    safeCallCallback(logger, "onToolCallRequestFinalized", baseOpts.onToolCallRequestFinalized, [
+                        predictionsPerformed,
+                        callId,
+                        {
+                            toolCallRequest: request,
+                            rawContent,
+                        },
+                    ]);
+                    let result;
+                    try {
+                        result = await tool.implementation(pushedRequest.arguments ?? {}, toolCallContext);
+                    }
+                    catch (error) {
+                        if (!(error instanceof UnimplementedToolError)) {
+                            throw error;
+                        }
+                        hasCalledUnimplementedTool = true;
                     }
-                    const result = await tool.implementation(parameters, toolCallContext);
                     let resultString;
                     if (result === undefined) {
                         resultString = "undefined";
@@ -16340,6 +16624,7 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
             predictionsPerformed >= baseOpts.maxPredictionRounds) {
             shouldContinue = false;
         }
+        shouldContinue &&= !hasCalledUnimplementedTool; // Stop loop if unimplemented tool was called.
     } while (shouldContinue);
     return new ActResult(predictionsPerformed, (performance.now() - startTime) / 1_000);
 }
@@ -16614,7 +16899,7 @@ const llmActionOptsSchema = llmPredictionConfigInputSchema
     preset: zod.z.string().optional(),
 });
 function splitActOpts(opts) {
-    const { onFirstToken, onPredictionFragment, onMessage, onRoundStart, onRoundEnd, onPredictionCompleted, onPromptProcessingProgress, onToolCallRequestStart, onToolCallRequestNameReceived, onToolCallRequestArgumentFragmentGenerated, onToolCallRequestEnd, onToolCallRequestFailure, onToolCallRequestDequeued, handleInvalidToolRequest, maxPredictionRounds, signal, preset, allowParallelToolExecution, ...config } = opts;
+    const { onFirstToken, onPredictionFragment, onMessage, onRoundStart, onRoundEnd, onPredictionCompleted, onPromptProcessingProgress, onToolCallRequestStart, onToolCallRequestNameReceived, onToolCallRequestArgumentFragmentGenerated, onToolCallRequestEnd, onToolCallRequestFinalized, onToolCallRequestFailure, onToolCallRequestDequeued, guardToolCall, handleInvalidToolRequest, maxPredictionRounds, signal, preset, allowParallelToolExecution, ...config } = opts;
     return [
         config,
         {
@@ -16629,8 +16914,10 @@ function splitActOpts(opts) {
             onToolCallRequestNameReceived,
             onToolCallRequestArgumentFragmentGenerated,
             onToolCallRequestEnd,
+            onToolCallRequestFinalized,
             onToolCallRequestFailure,
             onToolCallRequestDequeued,
+            guardToolCall,
             handleInvalidToolRequest,
             maxPredictionRounds,
             signal,
@@ -16681,6 +16968,9 @@ class LLMDynamicHandle extends DynamicHandle {
     internalPredict(history, predictionConfigStack, cancelEvent, extraOpts, onFragment, onFinished, onError) {
         let finished = false;
         let firstTokenTriggered = false;
+        let currentCallId = null;
+        let receivedEagerToolNameReporting = false;
+        let receivedToolArgumentsStreaming = false;
         const channel = this.port.createChannel("predict", {
             modelSpecifier: this.specifier,
             history,
@@ -16705,24 +16995,49 @@ class LLMDynamicHandle extends DynamicHandle {
                     break;
                 }
                 case "toolCallGenerationStart": {
-                    safeCallCallback(this.logger, "onToolCallGenerationStart", extraOpts.onToolCallRequestStart, []);
+                    if (currentCallId === null) {
+                        currentCallId = 0;
+                    }
+                    else {
+                        currentCallId++;
+                    }
+                    receivedEagerToolNameReporting = false;
+                    receivedToolArgumentsStreaming = false;
+                    safeCallCallback(this.logger, "onToolCallGenerationStart", extraOpts.onToolCallRequestStart, [currentCallId, { toolCallId: message.toolCallId }]);
                     break;
                 }
                 case "toolCallGenerationNameReceived": {
-                    safeCallCallback(this.logger, "onToolCallGenerationNameReceived", extraOpts.onToolCallRequestNameReceived, [message.name]);
+                    receivedEagerToolNameReporting = true;
+                    safeCallCallback(this.logger, "onToolCallGenerationNameReceived", extraOpts.onToolCallRequestNameReceived, [currentCallId ?? -1, message.name]);
                     break;
                 }
                 case "toolCallGenerationArgumentFragmentGenerated": {
-                    safeCallCallback(this.logger, "onToolCallGenerationArgumentFragmentGenerated", extraOpts.onToolCallRequestArgumentFragmentGenerated, [message.content]);
+                    receivedToolArgumentsStreaming = true;
+                    safeCallCallback(this.logger, "onToolCallGenerationArgumentFragmentGenerated", extraOpts.onToolCallRequestArgumentFragmentGenerated, [currentCallId ?? -1, message.content]);
                     break;
                 }
                 case "toolCallGenerationEnd": {
-                    safeCallCallback(this.logger, "onToolCallGenerationEnd", extraOpts.onToolCallRequestEnd, [{ toolCallRequest: message.toolCallRequest, rawContent: message.rawContent }]);
+                    if (!receivedEagerToolNameReporting) {
+                        // If eager name reporting not received, report it.
+                        safeCallCallback(this.logger, "onToolCallGenerationNameReceived", extraOpts.onToolCallRequestNameReceived, [currentCallId ?? -1, message.toolCallRequest.name]);
+                    }
+                    if (!receivedToolArgumentsStreaming) {
+                        // If arguments streaming not received, just pretend we have received all the
+                        // arguments as a single JSON
+                        safeCallCallback(this.logger, "onToolCallGenerationArgumentFragmentGenerated", extraOpts.onToolCallRequestArgumentFragmentGenerated, [
+                            currentCallId ?? -1,
+                            JSON.stringify(message.toolCallRequest.arguments ?? {}, null, 2),
+                        ]);
+                    }
+                    safeCallCallback(this.logger, "onToolCallGenerationEnd", extraOpts.onToolCallRequestEnd, [
+                        currentCallId ?? -1,
+                        { toolCallRequest: message.toolCallRequest, rawContent: message.rawContent },
+                    ]);
                     break;
                 }
                 case "toolCallGenerationFailed": {
                     const toolCallRequestError = new ToolCallRequestError(fromSerializedError(message.error).message, message.rawContent);
-                    safeCallCallback(this.logger, "onToolCallGenerationFailed", extraOpts.onToolCallRequestFailure, [toolCallRequestError]);
+                    safeCallCallback(this.logger, "onToolCallGenerationFailed", extraOpts.onToolCallRequestFailure, [currentCallId ?? -1, toolCallRequestError]);
                     break;
                 }
                 case "success": {
@@ -17033,7 +17348,7 @@ class LLMDynamicHandle extends DynamicHandle {
                         break;
                     }
                     case "toolCallGenerationStart": {
-                        handleToolCallGenerationStart();
+                        handleToolCallGenerationStart(message.toolCallId);
                         break;
                     }
                     case "toolCallGenerationNameReceived": {
@@ -17170,6 +17485,52 @@ class LLM extends LLMDynamicHandle {
     }
 }
 
+/** @public */
+class LLMNamespace extends ModelNamespace {
+    constructor() {
+        super(...arguments);
+        /** @internal */
+        this.namespace = "llm";
+        /** @internal */
+        this.defaultLoadConfig = {};
+        /** @internal */
+        this.loadModelConfigSchema = llmLoadModelConfigSchema;
+    }
+    /** @internal */
+    loadConfigToKVConfig(config) {
+        return llmLlamaMoeLoadConfigSchematics.buildPartialConfig({
+            "contextLength": config.contextLength,
+            "llama.evalBatchSize": config.evalBatchSize,
+            "llama.acceleration.offloadRatio": config.gpu?.ratio,
+            "load.gpuSplitConfig": convertGPUSettingToGPUSplitConfig(config.gpu),
+            "llama.flashAttention": config.flashAttention,
+            "llama.ropeFrequencyBase": numberToCheckboxNumeric(config.ropeFrequencyBase, 0, 0),
+            "llama.ropeFrequencyScale": numberToCheckboxNumeric(config.ropeFrequencyScale, 0, 0),
+            "llama.keepModelInMemory": config.keepModelInMemory,
+            "seed": numberToCheckboxNumeric(config.seed, -1, 0),
+            "llama.useFp16ForKVCache": config.useFp16ForKVCache,
+            "llama.tryMmap": config.tryMmap,
+            "numExperts": config.numExperts,
+            "llama.kCacheQuantizationType": cacheQuantizationTypeToCheckbox({
+                value: config.llamaKCacheQuantizationType,
+                falseDefault: "f16",
+            }),
+            "llama.vCacheQuantizationType": cacheQuantizationTypeToCheckbox({
+                value: config.llamaVCacheQuantizationType,
+                falseDefault: "f16",
+            }),
+        });
+    }
+    /** @internal */
+    createDomainSpecificModel(port, info, validator, logger) {
+        return new LLM(port, info, validator, logger);
+    }
+    /** @internal */
+    createDomainDynamicHandle(port, specifier, validator, logger) {
+        return new LLMDynamicHandle(port, specifier, validator, logger);
+    }
+}
+
 /**
  * Represents the result of a prediction from a generator plugin.
  *
@@ -17337,11 +17698,58 @@ class LLMGeneratorHandle {
     /** @internal */
     validator, 
     /** @internal */
+    associatedPredictionProcess, 
+    /** @internal */
     logger = new SimpleLogger(`LLMGeneratorHandle`)) {
         this.port = port;
         this.pluginIdentifier = pluginIdentifier;
         this.validator = validator;
+        this.associatedPredictionProcess = associatedPredictionProcess;
         this.logger = logger;
+        /**
+         * The identifier of the plugin that this handle is associated with.
+         */
+        this.identifier = this.pluginIdentifier;
+    }
+    getPluginConfigSpecifier(userSuppliedPluginConfig, userSuppliedWorkingDirectory, stack) {
+        if (this.associatedPredictionProcess === null) {
+            // If there is no associated prediction process, we can use the user-supplied config directly.
+            return {
+                type: "direct",
+                config: userSuppliedPluginConfig ?? emptyKVConfig,
+                workingDirectoryPath: userSuppliedWorkingDirectory ?? undefined,
+            };
+        }
+        // If there is an associated prediction process, we first need to make sure that the user has
+        // not supplied a plugin config or working directory, as these are not allowed in this case.
+        // (The plugin config/working directory of the prediction process will be used instead.)
+        if (userSuppliedPluginConfig !== undefined) {
+            throw makeTitledPrettyError("Cannot use plugin config with prediction process", text `
+          You cannot provide a plugin config to the generator handle when it is associated with a
+          prediction process. The plugin config that was configured for the prediction process will
+          be used instead.
+
+          If you want to use a different plugin config, you will need to create a separate
+          GeneratorHandle instead.
+        `, stack);
+        }
+        if (userSuppliedWorkingDirectory !== undefined) {
+            throw makeTitledPrettyError("Cannot use working directory with prediction process", text `
+          You cannot provide a working directory to the generator handle when it is associated with
+          a prediction process. The working directory that was configured for the prediction process
+          will be used instead.
+
+          If you want to use a different working directory, you will need to create a separate
+          GeneratorHandle instead.
+        `, stack);
+        }
+        // If we reach here, we can safely return the plugin config specifier for the prediction
+        // process.
+        return {
+            type: "predictionProcess",
+            pci: this.associatedPredictionProcess.pci,
+            token: this.associatedPredictionProcess.token,
+        };
     }
     /**
      * Use the generator to produce a response based on the given history.
@@ -17349,7 +17757,7 @@ class LLMGeneratorHandle {
     respond(chat, opts = {}) {
         const stack = getCurrentStack(1);
         [chat, opts] = this.validator.validateMethodParamsOrThrow("LLMGeneratorHandle", "respond", ["chat", "opts"], [chatHistoryLikeSchema, llmGeneratorPredictionOptsSchema], [chat, opts], stack);
-        const { onFirstToken, onPredictionFragment, onMessage, signal, pluginConfig = emptyKVConfig, workingDirectory, } = opts;
+        const { onFirstToken, onPredictionFragment, onMessage, signal, pluginConfig, workingDirectory, } = opts;
         let resolved = false;
         let firstTokenTriggered = false;
         const cancelEvent = new CancelEvent();
@@ -17367,9 +17775,8 @@ class LLMGeneratorHandle {
         });
         const channel = this.port.createChannel("generateWithGenerator", {
             pluginIdentifier: this.pluginIdentifier,
-            pluginConfigStack: singleLayerKVConfigStackOf("apiOverride", pluginConfig),
+            pluginConfigSpecifier: this.getPluginConfigSpecifier(pluginConfig, workingDirectory, stack),
             tools: [],
-            workingDirectoryPath: workingDirectory ?? null,
             history: accessMaybeMutableInternals(Chat.from(chat))._internalGetData(),
         }, message => {
             const messageType = message.type;
@@ -17417,7 +17824,7 @@ class LLMGeneratorHandle {
         const startTime = performance.now();
         const stack = getCurrentStack(1);
         [chat, opts] = this.validator.validateMethodParamsOrThrow("LLMGeneratorHandle", "act", ["chat", "opts"], [chatHistoryLikeSchema, llmGeneratorActOptsSchema], [chat, opts], stack);
-        const { pluginConfig = emptyKVConfig, workingDirectory = null, ...baseOpts } = opts;
+        const { pluginConfig, workingDirectory, ...baseOpts } = opts;
         const toolDefinitions = tools.map(toolToLLMTool);
         return await internalAct(chat, tools, baseOpts, stack, this.logger, startTime, 
         // Implementation of the prediction function. This performs the prediction by creating a
@@ -17426,9 +17833,8 @@ class LLMGeneratorHandle {
             // Use predict channel
             const channel = this.port.createChannel("generateWithGenerator", {
                 pluginIdentifier: this.pluginIdentifier,
-                pluginConfigStack: singleLayerKVConfigStackOf("apiOverride", pluginConfig),
+                pluginConfigSpecifier: this.getPluginConfigSpecifier(pluginConfig, workingDirectory, stack),
                 tools: allowTools ? toolDefinitions : [],
-                workingDirectoryPath: workingDirectory,
                 history,
             }, message => {
                 const messageType = message.type;
@@ -17442,7 +17848,7 @@ class LLMGeneratorHandle {
                         break;
                     }
                     case "toolCallGenerationStart": {
-                        handleToolCallGenerationStart();
+                        handleToolCallGenerationStart(message.toolCallId);
                         break;
                     }
                     case "toolCallGenerationNameReceived": {
@@ -17483,59 +17889,6 @@ class LLMGeneratorHandle {
     }
 }
 
-/** @public */
-class LLMNamespace extends ModelNamespace {
-    constructor() {
-        super(...arguments);
-        /** @internal */
-        this.namespace = "llm";
-        /** @internal */
-        this.defaultLoadConfig = {};
-        /** @internal */
-        this.loadModelConfigSchema = llmLoadModelConfigSchema;
-    }
-    /** @internal */
-    loadConfigToKVConfig(config) {
-        return llmLlamaMoeLoadConfigSchematics.buildPartialConfig({
-            "contextLength": config.contextLength,
-            "llama.evalBatchSize": config.evalBatchSize,
-            "llama.acceleration.offloadRatio": config.gpu?.ratio,
-            "load.gpuSplitConfig": convertGPUSettingToGPUSplitConfig(config.gpu),
-            "llama.flashAttention": config.flashAttention,
-            "llama.ropeFrequencyBase": numberToCheckboxNumeric(config.ropeFrequencyBase, 0, 0),
-            "llama.ropeFrequencyScale": numberToCheckboxNumeric(config.ropeFrequencyScale, 0, 0),
-            "llama.keepModelInMemory": config.keepModelInMemory,
-            "seed": numberToCheckboxNumeric(config.seed, -1, 0),
-            "llama.useFp16ForKVCache": config.useFp16ForKVCache,
-            "llama.tryMmap": config.tryMmap,
-            "numExperts": config.numExperts,
-            "llama.kCacheQuantizationType": cacheQuantizationTypeToCheckbox({
-                value: config.llamaKCacheQuantizationType,
-                falseDefault: "f16",
-            }),
-            "llama.vCacheQuantizationType": cacheQuantizationTypeToCheckbox({
-                value: config.llamaVCacheQuantizationType,
-                falseDefault: "f16",
-            }),
-        });
-    }
-    /** @internal */
-    createDomainSpecificModel(port, info, validator, logger) {
-        return new LLM(port, info, validator, logger);
-    }
-    /** @internal */
-    createDomainDynamicHandle(port, specifier, validator, logger) {
-        return new LLMDynamicHandle(port, specifier, validator, logger);
-    }
-    /**
-     * @experimental [EXP-GEN-PREDICT] Using generator plugins programmatically is still in development.
-     * This may change in the future without warning.
-     */
-    createGeneratorHandle(pluginIdentifier) {
-        return new LLMGeneratorHandle(this.port, pluginIdentifier, this.validator, this.logger);
-    }
-}
-
 const generatorSchema = zod.z.function();
 
 /**
@@ -17691,8 +18044,8 @@ class GeneratorController extends BaseController {
      * successfully generated tool calls, or a `toolCallGenerationFailed` call for
      * failed tool calls.
      */
-    toolCallGenerationStarted() {
-        this.connector.toolCallGenerationStarted();
+    toolCallGenerationStarted({ toolCallId, } = {}) {
+        this.connector.toolCallGenerationStarted(toolCallId);
     }
     /**
      * Use this function to report that the name of the tool call has been generated. This function
@@ -17738,7 +18091,7 @@ class GeneratorController extends BaseController {
     }
 }
 
-var __addDisposableResource = (globalThis && globalThis.__addDisposableResource) || function (env, value, async) {
+var __addDisposableResource$1 = (globalThis && globalThis.__addDisposableResource) || function (env, value, async) {
     if (value !== null && value !== void 0) {
         if (typeof value !== "object" && typeof value !== "function") throw new TypeError("Object expected.");
         var dispose, inner;
@@ -17760,7 +18113,7 @@ var __addDisposableResource = (globalThis && globalThis.__addDisposableResource)
     }
     return value;
 };
-var __disposeResources = (globalThis && globalThis.__disposeResources) || (function (SuppressedError) {
+var __disposeResources$1 = (globalThis && globalThis.__disposeResources) || (function (SuppressedError) {
     return function (env) {
         function fail(e) {
             env.error = env.hasError ? new SuppressedError(e, env.error, "An error was suppressed during disposal.") : e;
@@ -17861,12 +18214,12 @@ class ProcessingConnector {
         // argument.
         return Chat.createRaw(chatHistoryData, /* mutable */ false).asMutableCopy();
     }
-    async getOrLoadModel() {
-        const result = await this.pluginsPort.callRpc("processingGetOrLoadModel", {
+    async getOrLoadTokenSource() {
+        const result = await this.pluginsPort.callRpc("processingGetOrLoadTokenSource", {
             pci: this.processingContextIdentifier,
             token: this.token,
         });
-        return result.identifier;
+        return result.tokenSourceIdentifier;
     }
     async hasStatus() {
         return await this.pluginsPort.callRpc("processingHasStatus", {
@@ -17893,7 +18246,7 @@ class ProcessingConnector {
  */
 class ProcessingController extends BaseController {
     /** @internal */
-    constructor(client, pluginConfig, globalPluginConfig, workingDirectoryPath, 
+    constructor(client, pluginConfig, globalPluginConfig, workingDirectoryPath, enabledPluginInfos, 
     /** @internal */
     connector, 
     /** @internal */
@@ -17905,27 +18258,10 @@ class ProcessingController extends BaseController {
      */
     shouldIncludeCurrentInHistory) {
         super(client, connector.abortSignal, pluginConfig, globalPluginConfig, workingDirectoryPath);
+        this.enabledPluginInfos = enabledPluginInfos;
         this.connector = connector;
         this.config = config;
         this.shouldIncludeCurrentInHistory = shouldIncludeCurrentInHistory;
-        this.model = Object.freeze({
-            getOrLoad: async () => {
-                const identifier = await this.connector.getOrLoadModel();
-                const model = await this.client.llm.model(identifier);
-                // Don't use the server session config for this model
-                model.internalIgnoreServerSessionConfig = true;
-                // Inject the prediction config
-                model.internalKVConfigStack = {
-                    layers: [
-                        {
-                            layerName: "conversationSpecific",
-                            config: this.config,
-                        },
-                    ],
-                };
-                return model;
-            },
-        });
         this.processingControllerHandle = {
             abortSignal: connector.abortSignal,
             sendUpdate: update => {
@@ -18025,8 +18361,40 @@ class ProcessingController extends BaseController {
     debug(...messages) {
         this.createDebugInfoBlock(concatenateDebugMessages(...messages));
     }
-    getPredictionConfig() {
-        return kvConfigToLLMPredictionConfig(this.config);
+    /**
+     * Gets the token source associated with this prediction process (i.e. what the user has selected
+     * on the top navigation bar).
+     *
+     * The token source can either be a model or a generator plugin. In both cases, the returned
+     * object will contain a ".act" and a ".respond" method, which can be used to generate text.
+     *
+     * The token source is already pre-configured to use user's prediction config - you don't need to
+     * pass through any additional configuration.
+     */
+    async tokenSource() {
+        const tokenSourceIdentifier = await this.connector.getOrLoadTokenSource();
+        const tokenSourceIdentifierType = tokenSourceIdentifier.type;
+        switch (tokenSourceIdentifierType) {
+            case "model": {
+                const model = await this.client.llm.model(tokenSourceIdentifier.identifier);
+                // Don't use the server session config for this model
+                model.internalIgnoreServerSessionConfig = true;
+                // Inject the prediction config
+                model.internalKVConfigStack = {
+                    layers: [
+                        {
+                            layerName: "conversationSpecific",
+                            config: this.config,
+                        },
+                    ],
+                };
+                return model;
+            }
+            case "generator": {
+                const generator = this.client.plugins.createGeneratorHandleAssociatedWithPredictionProcess(tokenSourceIdentifier.pluginIdentifier, this.connector.processingContextIdentifier, this.connector.token);
+                return generator;
+            }
+        }
     }
     /**
      * Sets the sender name for this message. The sender name shown above the message in the chat.
@@ -18105,6 +18473,31 @@ class ProcessingController extends BaseController {
         const toolStatusController = new PredictionProcessToolStatusController(this.processingControllerHandle, id, initialStatus);
         return toolStatusController;
     }
+    /**
+     * Starts a tool use session with tools available in the prediction process. Note, this method
+     * should be used with "Explicit Resource Management". That is, you should use it like so:
+     *
+     * ```typescript
+     * using toolUseSession = await ctl.startToolUseSession();
+     * // ^ Notice the `using` keyword here.
+     * ```
+     *
+     * If you do not `using`, you should call `toolUseSession[Symbol.dispose]()` after you are done.
+     *
+     * If you don't, lmstudio-js will close the session upon the end of the prediction step
+     * automatically. However, it is not recommended.
+     *
+     * @public
+     * @deprecated WIP
+     */
+    async startToolUseSession() {
+        const identifiersOfPluginsWithTools = this.enabledPluginInfos
+            .filter(({ hasToolsProvider }) => hasToolsProvider)
+            .map(({ identifier }) => identifier);
+        return await this.client.plugins.startToolUseSessionUsingPredictionProcess(
+        // We start a tool use session with all the plugins that have tools available
+        identifiersOfPluginsWithTools, this.connector.processingContextIdentifier, this.connector.token);
+    }
 }
 /**
  * Controller for a status block in the prediction process.
@@ -18303,7 +18696,7 @@ class PredictionProcessContentBlockController {
     async pipeFrom(prediction) {
         const env_1 = { stack: [], error: void 0, hasError: false };
         try {
-            const cleaner = __addDisposableResource(env_1, new Cleaner(), false);
+            const cleaner = __addDisposableResource$1(env_1, new Cleaner(), false);
             const abortListener = () => {
                 prediction.cancel();
             };
@@ -18330,7 +18723,7 @@ class PredictionProcessContentBlockController {
             env_1.hasError = true;
         }
         finally {
-            __disposeResources(env_1);
+            __disposeResources$1(env_1);
         }
     }
 }
@@ -18411,10 +18804,11 @@ class GeneratorConnectorImpl {
             opts: opts,
         });
     }
-    toolCallGenerationStarted() {
+    toolCallGenerationStarted(toolCallId) {
         this.channel.send({
             type: "toolCallGenerationStarted",
             taskId: this.taskId,
+            toolCallId,
         });
     }
     toolCallGenerationNameReceived(toolName) {
@@ -18477,7 +18871,7 @@ class PluginSelfRegistrationHost {
                     const abortController = new AbortController();
                     const connector = new ProcessingConnector(this.port, abortController.signal, message.pci, message.token, taskLogger);
                     const input = ChatMessage.createRaw(message.input, /* mutable */ false);
-                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, connector, message.config, 
+                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, message.enabledPluginInfos, connector, message.config, 
                     /* shouldIncludeInputInHistory */ false);
                     tasks.set(message.taskId, {
                         cancel: () => {
@@ -18547,7 +18941,7 @@ class PluginSelfRegistrationHost {
         }, { stack });
     }
     /**
-     * Sets the promptPreprocessor to be used by the plugin represented by this client.
+     * Sets the prediction loop handler to be used by the plugin represented by this client.
      *
      * @deprecated [DEP-PLUGIN-PREDICTION-LOOP-HANDLER] Prediction loop handler support is still in
      * development. Stay tuned for updates.
@@ -18565,7 +18959,7 @@ class PluginSelfRegistrationHost {
                     taskLogger.info(`New prediction loop handling request received.`);
                     const abortController = new AbortController();
                     const connector = new ProcessingConnector(this.port, abortController.signal, message.pci, message.token, taskLogger);
-                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, connector, message.config, 
+                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, message.enabledPluginInfos, connector, message.config, 
                     /* shouldIncludeInputInHistory */ true);
                     tasks.set(message.taskId, {
                         cancel: () => {
@@ -18944,6 +19338,309 @@ class PluginSelfRegistrationHost {
     }
 }
 
+var __addDisposableResource = (globalThis && globalThis.__addDisposableResource) || function (env, value, async) {
+    if (value !== null && value !== void 0) {
+        if (typeof value !== "object" && typeof value !== "function") throw new TypeError("Object expected.");
+        var dispose, inner;
+        if (async) {
+            if (!Symbol.asyncDispose) throw new TypeError("Symbol.asyncDispose is not defined.");
+            dispose = value[Symbol.asyncDispose];
+        }
+        if (dispose === void 0) {
+            if (!Symbol.dispose) throw new TypeError("Symbol.dispose is not defined.");
+            dispose = value[Symbol.dispose];
+            if (async) inner = dispose;
+        }
+        if (typeof dispose !== "function") throw new TypeError("Object not disposable.");
+        if (inner) dispose = function() { try { inner.call(this); } catch (e) { return Promise.reject(e); } };
+        env.stack.push({ value: value, dispose: dispose, async: async });
+    }
+    else if (async) {
+        env.stack.push({ async: true });
+    }
+    return value;
+};
+var __disposeResources = (globalThis && globalThis.__disposeResources) || (function (SuppressedError) {
+    return function (env) {
+        function fail(e) {
+            env.error = env.hasError ? new SuppressedError(e, env.error, "An error was suppressed during disposal.") : e;
+            env.hasError = true;
+        }
+        var r, s = 0;
+        function next() {
+            while (r = env.stack.pop()) {
+                try {
+                    if (!r.async && s === 1) return s = 0, env.stack.push(r), Promise.resolve().then(next);
+                    if (r.dispose) {
+                        var result = r.dispose.call(r.value);
+                        if (r.async) return s |= 2, Promise.resolve(result).then(next, function(e) { fail(e); return next(); });
+                    }
+                    else s |= 1;
+                }
+                catch (e) {
+                    fail(e);
+                }
+            }
+            if (s === 1) return env.hasError ? Promise.reject(env.error) : Promise.resolve();
+            if (env.hasError) throw env.error;
+        }
+        return next();
+    };
+})(typeof SuppressedError === "function" ? SuppressedError : function (error, suppressed, message) {
+    var e = new Error(message);
+    return e.name = "SuppressedError", e.error = error, e.suppressed = suppressed, e;
+});
+/**
+ * Represents a tool use session backed by a single plugin. Don't construct this class yourself.
+ *
+ * @public
+ */
+class SingleRemoteToolUseSession {
+    static async create(pluginsPort, pluginIdentifier, pluginConfigSpecifier, logger, stack) {
+        const session = new SingleRemoteToolUseSession(pluginsPort, pluginIdentifier, pluginConfigSpecifier, logger);
+        await session.init(stack);
+        return session;
+    }
+    constructor(pluginsPort, pluginIdentifier, pluginConfigSpecifier, logger) {
+        this.pluginsPort = pluginsPort;
+        this.pluginIdentifier = pluginIdentifier;
+        this.pluginConfigSpecifier = pluginConfigSpecifier;
+        this.logger = logger;
+        this.status = "initializing";
+        /**
+         * Whether this session is "poisoned". A session is poisoned either when the underlying channel
+         * has errored/closed.
+         */
+        this.poison = null;
+        /**
+         * Map to track all the ongoing tool calls.
+         */
+        this.ongoingToolCalls = new Map();
+        this.callIdGiver = new IdGiver(0);
+    }
+    async init(stack) {
+        const { promise: initPromise, resolve: resolveInit, reject: rejectInit } = makePromise();
+        const channel = this.pluginsPort.createChannel("startToolUseSession", {
+            pluginIdentifier: this.pluginIdentifier,
+            pluginConfigSpecifier: this.pluginConfigSpecifier,
+        }, message => {
+            const messageType = message.type;
+            switch (messageType) {
+                // Upon receiving session ready, mark self as ready and resolve the promise.
+                case "sessionReady": {
+                    if (this.status !== "initializing") {
+                        this.logger.error("Received sessionReady message while not initializing");
+                        return;
+                    }
+                    this.status = "ready";
+                    resolveInit();
+                    this.tools = message.toolDefinitions.map(toolDefinition => this.makeTool(toolDefinition));
+                    break;
+                }
+                case "toolCallComplete": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.resolve(message.result);
+                    break;
+                }
+                case "toolCallError": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.reject(fromSerializedError(message.error));
+                    break;
+                }
+                case "toolCallStatus": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.reportStatus(message.statusText);
+                    break;
+                }
+                case "toolCallWarn": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.reportWarning(message.warnText);
+                    break;
+                }
+                default: {
+                    const exhaustiveCheck = messageType;
+                    this.logger.warn(`Received unexpected message type in tool use session: ${exhaustiveCheck}`);
+                }
+            }
+        }, { stack });
+        channel.onError.subscribeOnce(error => {
+            if (this.status === "initializing") {
+                // If still initializing, reject the promise with the error.
+                rejectInit(error);
+            }
+            else {
+                this.logger.error("Tool use session error.", error);
+                this.poison = error;
+            }
+            // Reject all ongoing tool calls with the error.
+            for (const ongoingCall of this.ongoingToolCalls.values()) {
+                ongoingCall.reject(error);
+            }
+            this.status = "disposed";
+        });
+        channel.onClose.subscribeOnce(() => {
+            let error;
+            if (this.status === "initializing") {
+                // If still initializing, reject the promise with the error.
+                error = new Error("Tool use session channel closed unexpectedly during initialization.");
+                rejectInit(error);
+            }
+            else {
+                error = new Error("Tool use session has already ended.");
+                // We don't print an error here because channel can close normally. We only poison this
+                // session so it throws the error when used.
+                this.poison = error;
+            }
+            // Reject all ongoing tool calls with the error.
+            for (const ongoingCall of this.ongoingToolCalls.values()) {
+                ongoingCall.reject(error);
+            }
+            this.status = "disposed";
+        });
+        this.channel = channel;
+        await initPromise;
+    }
+    [Symbol.dispose]() {
+        // As long as we are not already disposed, we send a discard message to the channel.
+        if (this.status !== "disposed") {
+            this.channel.send({ type: "discardSession" });
+            this.status = "disposed";
+            const error = new Error("Session disposed by client.");
+            // Reject all ongoing tool calls with the error.
+            for (const ongoingCall of this.ongoingToolCalls.values()) {
+                ongoingCall.reject(error);
+            }
+            this.poison = error;
+        }
+    }
+    makeTool(toolDefinition) {
+        return internalCreateRemoteTool({
+            name: toolDefinition.function.name,
+            description: toolDefinition.function.description ?? "",
+            pluginIdentifier: this.pluginIdentifier,
+            parametersJsonSchema: toolDefinition.function.parameters ?? {},
+            implementation: async (args, ctx) => {
+                const env_1 = { stack: [], error: void 0, hasError: false };
+                try {
+                    // We now need to provide an implementation that basically proxies the execution of the tool
+                    // to the backend.
+                    if (this.poison !== null) {
+                        // If the session is already poisoned, throw the error.
+                        throw this.poison;
+                    }
+                    // Handling the case where the request is already aborted before we start the tool call.
+                    if (ctx.signal.aborted) {
+                        throw ctx.signal.reason;
+                    }
+                    const callId = this.callIdGiver.next();
+                    const { promise, resolve, reject } = makePromise();
+                    const cleaner = __addDisposableResource(env_1, new Cleaner(), false);
+                    this.ongoingToolCalls.set(callId, {
+                        callId,
+                        resolve,
+                        reject,
+                        reportStatus: status => ctx.status(status),
+                        reportWarning: warning => ctx.warn(warning),
+                    });
+                    cleaner.register(() => {
+                        this.ongoingToolCalls.delete(callId);
+                    });
+                    this.channel.send({
+                        type: "callTool",
+                        callId,
+                        name: toolDefinition.function.name,
+                        arguments: args,
+                    });
+                    ctx.signal.addEventListener("abort", () => {
+                        if (this.status === "disposed") {
+                            return;
+                        }
+                        this.channel.send({
+                            type: "abortToolCall",
+                            callId,
+                        });
+                        reject(ctx.signal.reason);
+                    }, { once: true });
+                    return await promise;
+                }
+                catch (e_1) {
+                    env_1.error = e_1;
+                    env_1.hasError = true;
+                }
+                finally {
+                    __disposeResources(env_1);
+                }
+            },
+        });
+    }
+}
+/**
+ * Represents a tool use session backed by multiple plugins. Don't construct this class yourself.
+ *
+ * @public
+ */
+class MultiRemoteToolUseSession {
+    static async createUsingPredictionProcess(pluginsPort, pluginIdentifiers, predictionContextIdentifier, token, logger, stack) {
+        // Start initializing all the sessions in parallel. This is OK because usually all the plugins
+        // are already loaded for the prediction process anyway.
+        const results = await Promise.allSettled(pluginIdentifiers.map(pluginIdentifier => SingleRemoteToolUseSession.create(pluginsPort, pluginIdentifier, {
+            type: "predictionProcess",
+            pci: predictionContextIdentifier,
+            token,
+        }, logger, stack)));
+        const failed = results.filter(result => result.status === "rejected");
+        if (failed.length > 0) {
+            // Some sessions failed to initialize. We need to terminate all the sessions that
+            // successfully initialized.
+            for (const result of results) {
+                if (result.status === "fulfilled") {
+                    try {
+                        result.value[Symbol.dispose]();
+                    }
+                    catch (error) {
+                        logger.error("Failed to dispose a session after initialization failure.", error);
+                    }
+                }
+            }
+            throw new AggregateError(failed.map(result => result.reason), "Failed to initialize some tool use sessions.");
+        }
+        return new MultiRemoteToolUseSession(results.map(result => result.value), logger);
+    }
+    constructor(sessions, logger) {
+        this.sessions = sessions;
+        this.logger = logger;
+        this.tools = [];
+        this.tools = sessions.flatMap(session => session.tools);
+    }
+    [Symbol.dispose]() {
+        // Dispose all the sessions.
+        for (const session of this.sessions) {
+            try {
+                session[Symbol.dispose]();
+            }
+            catch (error) {
+                this.logger.error("Failed to dispose a session.", error);
+            }
+        }
+    }
+}
+
+const pluginToolsOptsSchema = zod.z.object({
+    pluginConfig: kvConfigSchema.optional(),
+    workingDirectory: zod.z.string().optional(),
+});
 const registerDevelopmentPluginOptsSchema = zod.z.object({
     manifest: pluginManifestSchema,
 });
@@ -19018,6 +19715,66 @@ class PluginsNamespace {
     getSelfRegistrationHost() {
         return new PluginSelfRegistrationHost(this.port, this.client, this.rootLogger, this.validator);
     }
+    /**
+     * Starts a tool use session use any config specifier.
+     */
+    async internalStartToolUseSession(pluginIdentifier, pluginConfigSpecifier, _stack) {
+        return await SingleRemoteToolUseSession.create(this.port, pluginIdentifier, pluginConfigSpecifier, this.logger);
+    }
+    /**
+     * Start a tool use session with a plugin. Note, this method must be used with "Explicit Resource
+     * Management". That is, you should use it like so:
+     *
+     * ```typescript
+     * using pluginTools = await client.plugins.pluginTools("owner/name", { ... });
+     * // ^ Notice the `using` keyword here.
+     * ```
+     *
+     * If you do not use `using`, you must call `pluginTools[Symbol.dispose]()` after you are done.
+     * Otherwise, there will be a memory leak and the plugins you requested tools from will be loaded
+     * indefinitely.
+     *
+     * @experimental [EXP-USE-USE-PLUGIN-TOOLS] Using tools from other applications is still in
+     * development. This may change in the future without warning.
+     */
+    async pluginTools(pluginIdentifier, opts = {}) {
+        const stack = getCurrentStack(1);
+        [pluginIdentifier, opts] = this.validator.validateMethodParamsOrThrow("plugins", "pluginTools", ["pluginIdentifier", "opts"], [artifactIdentifierSchema, pluginToolsOptsSchema], [pluginIdentifier, opts], stack);
+        return await this.internalStartToolUseSession(pluginIdentifier, {
+            type: "direct",
+            config: opts.pluginConfig ?? emptyKVConfig,
+            workingDirectoryPath: opts.workingDirectory,
+        });
+    }
+    /**
+     * Start a tool use session associated with a prediction process.
+     *
+     * This method is used internally by processing controllers and will be stripped by the internal
+     * tag.
+     *
+     * @internal
+     */
+    async startToolUseSessionUsingPredictionProcess(pluginIdentifiers, predictionContextIdentifier, token, stack) {
+        return await MultiRemoteToolUseSession.createUsingPredictionProcess(this.port, pluginIdentifiers, predictionContextIdentifier, token, this.logger, stack);
+    }
+    /**
+     * @experimental [EXP-GEN-PREDICT] Using generator plugins programmatically is still in
+     * development. This may change in the future without warning.
+     */
+    createGeneratorHandle(pluginIdentifier) {
+        return new LLMGeneratorHandle(this.port, pluginIdentifier, this.validator, null, this.logger);
+    }
+    /**
+     * Creates a generator handle that is already associated with a prediction process.
+     *
+     * This method is used internally by the processing controllers to create generator handles. It is
+     * marked as internal and will be stripped.
+     *
+     * @internal
+     */
+    createGeneratorHandleAssociatedWithPredictionProcess(pluginIdentifier, predictionContextIdentifier, token) {
+        return new LLMGeneratorHandle(this.port, pluginIdentifier, this.validator, { pci: predictionContextIdentifier, token }, this.logger);
+    }
 }
 
 const artifactDownloadPlannerDownloadOptsSchema = zod.z.object({
@@ -19639,7 +20396,7 @@ class LMStudioClient {
         }
         // On browser, those apiServerPorts are not accessible anyway. We will just try to see if we can
         // reach the server on 127.0.0.1:1234 (the default port).
-        if (process$1.browser) {
+        if (process.browser) {
             try {
                 this.isLocalhostWithGivenPortLMStudioServer(1234);
                 return "ws://127.0.0.1:1234";
@@ -19780,3 +20537,4 @@ exports.kvValueTypesLibrary = kvValueTypesLibrary;
 exports.rawFunctionTool = rawFunctionTool;
 exports.text = text;
 exports.tool = tool;
+exports.unimplementedRawFunctionTool = unimplementedRawFunctionTool;
diff --git a/node_modules/@lmstudio/sdk/dist/index.d.ts b/node_modules/@lmstudio/sdk/dist/index.d.ts
index c982319..bf93a26 100644
--- a/node_modules/@lmstudio/sdk/dist/index.d.ts
+++ b/node_modules/@lmstudio/sdk/dist/index.d.ts
@@ -1329,13 +1329,13 @@ declare const clientToServerMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
 }, "strip", z.ZodTypeAny, {
     creationParameter: SerializedOpaque<any>;
     type: "channelCreate";
-    channelId: number;
     endpoint: string;
+    channelId: number;
 }, {
     creationParameter: SerializedOpaque<any>;
     type: "channelCreate";
-    channelId: number;
     endpoint: string;
+    channelId: number;
 }>, z.ZodObject<{
     type: z.ZodLiteral<"channelSend">;
     channelId: z.ZodNumber;
@@ -1371,13 +1371,13 @@ declare const clientToServerMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
 }, "strip", z.ZodTypeAny, {
     parameter: SerializedOpaque<any>;
     type: "rpcCall";
-    callId: number;
     endpoint: string;
+    callId: number;
 }, {
     parameter: SerializedOpaque<any>;
     type: "rpcCall";
-    callId: number;
     endpoint: string;
+    callId: number;
 }>, z.ZodObject<{
     type: z.ZodLiteral<"signalSubscribe">;
     creationParameter: z.ZodType<SerializedOpaque<any>, z.ZodTypeDef, SerializedOpaque<any>>;
@@ -1386,13 +1386,13 @@ declare const clientToServerMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
 }, "strip", z.ZodTypeAny, {
     creationParameter: SerializedOpaque<any>;
     type: "signalSubscribe";
-    subscribeId: number;
     endpoint: string;
+    subscribeId: number;
 }, {
     creationParameter: SerializedOpaque<any>;
     type: "signalSubscribe";
-    subscribeId: number;
     endpoint: string;
+    subscribeId: number;
 }>, z.ZodObject<{
     type: z.ZodLiteral<"signalUnsubscribe">;
     subscribeId: z.ZodNumber;
@@ -1410,13 +1410,13 @@ declare const clientToServerMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
 }, "strip", z.ZodTypeAny, {
     creationParameter: SerializedOpaque<any>;
     type: "writableSignalSubscribe";
-    subscribeId: number;
     endpoint: string;
+    subscribeId: number;
 }, {
     creationParameter: SerializedOpaque<any>;
     type: "writableSignalSubscribe";
-    subscribeId: number;
     endpoint: string;
+    subscribeId: number;
 }>, z.ZodObject<{
     type: z.ZodLiteral<"writableSignalUnsubscribe">;
     subscribeId: z.ZodNumber;
@@ -1704,13 +1704,13 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
         };
     };
 } & {
-    processingGetOrLoadModel: {
+    processingGetOrLoadTokenSource: {
         parameter: {
             pci: string;
             token: string;
         };
         returns: {
-            identifier: string;
+            tokenSourceIdentifier: TokenSourceIdentifier;
         };
     };
 } & {
@@ -1767,6 +1767,238 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
         returns: void;
     };
 }, {
+    startToolUseSession: {
+        creationParameter: {
+            pluginIdentifier: string;
+            pluginConfigSpecifier: {
+                type: "direct";
+                config: {
+                    fields: {
+                        key: string;
+                        value?: any;
+                    }[];
+                };
+                workingDirectoryPath?: string | undefined;
+            } | {
+                type: "predictionProcess";
+                pci: string;
+                token: string;
+            };
+        };
+        toServerPacket: {
+            type: "callTool";
+            name: string;
+            callId: number;
+            arguments?: any;
+        } | {
+            type: "abortToolCall";
+            callId: number;
+        } | {
+            type: "discardSession";
+        };
+        toClientPacket: {
+            type: "sessionReady";
+            toolDefinitions: {
+                function: {
+                    name: string;
+                    description?: string | undefined;
+                    parameters? /**
+                    * Client requests to discard the session. Upon calling this, the channel will be closed.
+                    */: {
+                        type: "object";
+                        properties: Record<string, any>;
+                        required?: string[] | undefined;
+                        additionalProperties?: boolean | undefined;
+                        $defs?: Record<string, any> | undefined;
+                    } | undefined;
+                };
+                type: "function";
+            }[];
+        } | {
+            type: "toolCallComplete";
+            callId: number;
+            result?: any;
+        } | {
+            type: "toolCallError";
+            error: {
+                title: string;
+                cause?: string | undefined;
+                suggestion?: string | undefined;
+                errorData?: Record<string, unknown> | undefined;
+                displayData?: {
+                    code: "generic.specificModelUnloaded";
+                } | {
+                    code: "generic.noModelMatchingQuery";
+                    query: {
+                        path?: string | undefined;
+                        identifier?: string | undefined;
+                        domain?: "llm" | "embedding" | "imageGen" | "transcription" | "tts" | undefined;
+                        vision?: boolean | undefined;
+                    };
+                    loadedModelsSample: string[];
+                    totalLoadedModels: number;
+                } | {
+                    code: "generic.pathNotFound";
+                    path: string;
+                    availablePathsSample: string[];
+                    totalModels: number;
+                } | {
+                    code: "generic.identifierNotFound";
+                    identifier: string;
+                    loadedModelsSample: string[];
+                    totalLoadedModels: number;
+                } | {
+                    code: "generic.domainMismatch";
+                    path: string;
+                    actualDomain: "llm" | "embedding" | "imageGen" | "transcription" | "tts";
+                    expectedDomain: "llm" | "embedding" | "imageGen" | "transcription" | "tts";
+                } | {
+                    code: "generic.engineDoesNotSupportFeature";
+                    feature: string;
+                    engineName: string;
+                    engineType: string;
+                    installedVersion: string;
+                    supportedVersion: string | null;
+                } | {
+                    code: "generic.presetNotFound";
+                    specifiedFuzzyPresetIdentifier: string;
+                    availablePresetsSample: {
+                        name: string;
+                        identifier: string;
+                    }[];
+                    totalAvailablePresets: number;
+                } | undefined;
+                stack?: string | undefined;
+                rootTitle?: string | undefined;
+            };
+            callId: number;
+        } | {
+            type: "toolCallStatus";
+            callId: number;
+            statusText: string;
+        } | {
+            type: "toolCallWarn";
+            callId: number;
+            warnText: string;
+        };
+    };
+} & {
+    generateWithGenerator: {
+        creationParameter: {
+            history: {
+                messages: ({
+                    content: ({
+                        type: "text";
+                        text: string;
+                    } | {
+                        type: "file";
+                        name: string;
+                        identifier: string;
+                        sizeBytes: number;
+                        fileType: "unknown" | "image" | "text/plain" | "application/pdf" | "application/word" | "text/other";
+                    } | {
+                        type: "toolCallRequest";
+                        toolCallRequest: FunctionToolCallRequest;
+                    })[];
+                    role: "assistant";
+                } | {
+                    content: ({
+                        type: "text";
+                        text: string;
+                    } | {
+                        type: "file";
+                        name: string;
+                        identifier: string;
+                        sizeBytes: number;
+                        fileType: "unknown" | "image" | "text/plain" | "application/pdf" | "application/word" | "text/other";
+                    })[];
+                    role: "user";
+                } | {
+                    content: ({
+                        type: "text";
+                        text: string;
+                    } | {
+                        type: "file";
+                        name: string;
+                        identifier: string;
+                        sizeBytes: number;
+                        fileType: "unknown" | "image" | "text/plain" | "application/pdf" | "application/word" | "text/other";
+                    })[];
+                    role: "system";
+                } | {
+                    content: {
+                        type: "toolCallResult";
+                        content: string;
+                        toolCallId?: string | undefined;
+                    }[];
+                    role: "tool";
+                })[];
+            };
+            pluginIdentifier: string;
+            pluginConfigSpecifier: {
+                type: "direct";
+                config: {
+                    fields: {
+                        key: string;
+                        value?: any;
+                    }[];
+                };
+                workingDirectoryPath?: string | undefined;
+            } | {
+                type: "predictionProcess";
+                pci: string;
+                token: string;
+            };
+            tools: {
+                function: {
+                    name: string;
+                    description?: string | undefined;
+                    parameters? /**
+                    * Client requests to discard the session. Upon calling this, the channel will be closed.
+                    */: {
+                        type: "object";
+                        properties: Record<string, any>;
+                        required?: string[] | undefined;
+                        additionalProperties?: boolean | undefined;
+                        $defs?: Record<string, any> | undefined;
+                    } | undefined;
+                };
+                type: "function";
+            }[];
+        };
+        toServerPacket: {
+            type: "cancel";
+        };
+        toClientPacket: {
+            type: "fragment";
+            fragment: {
+                content: string;
+                tokensCount: number;
+                containsDrafted: boolean;
+                reasoningType: "none" | "reasoning" | "reasoningStartTag" | "reasoningEndTag";
+            };
+        } | {
+            type: "promptProcessingProgress";
+            progress: number;
+        } | {
+            type: "toolCallGenerationStart";
+            toolCallId?: string | undefined;
+        } | {
+            type: "toolCallGenerationNameReceived";
+            name: string;
+        } | {
+            type: "toolCallGenerationArgumentFragmentGenerated";
+            content: string;
+        } | {
+            type: "toolCallGenerationEnd";
+            toolCallRequest: FunctionToolCallRequest;
+        } | {
+            type: "toolCallGenerationFailed";
+        } | {
+            type: "success";
+        };
+    };
+} & {
     registerDevelopmentPlugin: {
         creationParameter: {
             manifest: {
@@ -1907,7 +2139,6 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                     value?: any;
                 }[];
             };
-            workingDirectoryPath: string | null;
             taskId: string;
             input: {
                 content: ({
@@ -1968,6 +2199,8 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                     value?: any;
                 }[];
             };
+            workingDirectoryPath: string | null;
+            enabledPluginInfos: RemotePluginInfo[];
             pci: string;
             token: string;
         } | {
@@ -2047,7 +2280,6 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                     value?: any;
                 }[];
             };
-            workingDirectoryPath: string | null;
             taskId: string;
             pluginConfig: {
                 fields: {
@@ -2061,6 +2293,8 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                     value?: any;
                 }[];
             };
+            workingDirectoryPath: string | null;
+            enabledPluginInfos: RemotePluginInfo[];
             pci: string;
             token: string;
         } | {
@@ -2073,20 +2307,23 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
         creationParameter: void;
         toServerPacket: {
             type: "sessionInitialized";
-            sessionId: string;
             toolDefinitions: {
                 function: {
                     name: string;
                     description?: string | undefined;
-                    parameters?: {
+                    parameters? /**
+                    * Client requests to discard the session. Upon calling this, the channel will be closed.
+                    */: {
                         type: "object";
                         properties: Record<string, any>;
                         required?: string[] | undefined;
                         additionalProperties?: boolean | undefined;
+                        $defs?: Record<string, any> | undefined;
                     } | undefined;
                 };
                 type: "function";
             }[];
+            sessionId: string;
         } | {
             type: "sessionInitializationFailed";
             error: {
@@ -2143,8 +2380,8 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
             sessionId: string;
         } | {
             type: "toolCallComplete";
-            sessionId: string;
             callId: string;
+            sessionId: string;
             result?: any;
         } | {
             type: "toolCallError";
@@ -2199,22 +2436,21 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                 stack?: string | undefined;
                 rootTitle?: string | undefined;
             };
-            sessionId: string;
             callId: string;
+            sessionId: string;
         } | {
             type: "toolCallStatus";
-            sessionId: string;
             callId: string;
             statusText: string;
+            sessionId: string;
         } | {
             type: "toolCallWarn";
-            sessionId: string;
             callId: string;
             warnText: string;
+            sessionId: string;
         };
         toClientPacket: {
             type: "initSession";
-            workingDirectoryPath: string | null;
             pluginConfig: {
                 fields: {
                     key: string;
@@ -2227,20 +2463,21 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                     value?: any;
                 }[];
             };
+            workingDirectoryPath: string | null;
             sessionId: string;
         } | {
             type: "discardSession";
             sessionId: string;
         } | {
             type: "callTool";
-            sessionId: string;
             callId: string;
+            sessionId: string;
             toolName: string;
             parameters?: any;
         } | {
             type: "abortToolCall";
-            sessionId: string;
             callId: string;
+            sessionId: string;
         };
     };
 } & {
@@ -2314,6 +2551,7 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
         } | {
             type: "toolCallGenerationStarted";
             taskId: string;
+            toolCallId?: string | undefined;
         } | {
             type: "toolCallGenerationNameReceived";
             taskId: string;
@@ -2383,7 +2621,22 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
         };
         toClientPacket: {
             type: "generate";
-            workingDirectoryPath: string | null;
+            toolDefinitions: {
+                function: {
+                    name: string;
+                    description?: string | undefined;
+                    parameters? /**
+                    * Client requests to discard the session. Upon calling this, the channel will be closed.
+                    */: {
+                        type: "object";
+                        properties: Record<string, any>;
+                        required?: string[] | undefined;
+                        additionalProperties?: boolean | undefined;
+                        $defs?: Record<string, any> | undefined;
+                    } | undefined;
+                };
+                type: "function";
+            }[];
             taskId: string;
             input: {
                 messages: ({
@@ -2446,19 +2699,7 @@ declare function createPluginsBackendInterface(): BackendInterface<never, {
                     value?: any;
                 }[];
             };
-            toolDefinitions: {
-                function: {
-                    name: string;
-                    description?: string | undefined;
-                    parameters?: {
-                        type: "object";
-                        properties: Record<string, any>;
-                        required?: string[] | undefined;
-                        additionalProperties?: boolean | undefined;
-                    } | undefined;
-                };
-                type: "function";
-            }[];
+            workingDirectoryPath: string | null;
         } | {
             type: "abort";
             taskId: string;
@@ -2882,7 +3123,12 @@ export declare class GeneratorController extends BaseController {
      * successfully generated tool calls, or a `toolCallGenerationFailed` call for
      * failed tool calls.
      */
-    toolCallGenerationStarted(): void;
+    toolCallGenerationStarted({ toolCallId, }?: {
+        /**
+         * The LLM specific call id of the tool call.
+         */
+        toolCallId?: string;
+    }): void;
     /**
      * Use this function to report that the name of the tool call has been generated. This function
      * should only be called once for each `toolCallGenerationStarted`.
@@ -3001,6 +3247,47 @@ export declare type GPUSetting = {
     disabledGpus?: number[];
 };
 
+/**
+ * Controller object used to allow/modify/deny a tool call.
+ */
+declare class GuardToolCallController {
+    readonly toolCallRequest: ToolCallRequest;
+    readonly tool: Tool;
+    readonly resultContainer: [result: GuardToolCallResult | null];
+    /**
+     * Don't construct this object yourself.
+     */
+    constructor(toolCallRequest: ToolCallRequest, tool: Tool, resultContainer: [result: GuardToolCallResult | null]);
+    private assertNoResultYet;
+    /**
+     * Allows the tool call to proceed without any modifications.
+     */
+    allow: () => void;
+    /**
+     * Allows the tool call to proceed, but overrides the parameters with the provided ones.
+     */
+    allowAndOverrideParameters: (newParameters: Record<string, any>) => void;
+    /**
+     * Denys the tool call with a specified reason. This will not interrupt the `.act` call. Instead,
+     * the reason you provide will be provided to the model as the tool call result.
+     *
+     * If `reason` is not provided, a generic default reason will be used.
+     *
+     * If you wish to immediately fail the `.act` call, you can throw an error instead.
+     */
+    deny: (reason?: string) => void;
+}
+
+declare type GuardToolCallResult = {
+    type: "allow";
+} | {
+    type: "allowAndOverrideParameters";
+    parameters: Record<string, any>;
+} | {
+    type: "deny";
+    reason?: string;
+};
+
 /**
  * Represents a download source for a Hugging Face model.
  *
@@ -3456,6 +3743,7 @@ export declare const kvValueTypesLibrary: KVFieldValueTypeLibrary<{
                         properties: Record<string, any>;
                         required?: string[] | undefined;
                         additionalProperties?: boolean | undefined;
+                        $defs?: Record<string, any> | undefined;
                     } | undefined;
                 };
                 type: "function";
@@ -3845,12 +4133,23 @@ export declare interface LLMActBaseOpts<TPredictionResult> {
      * @experimental [EXP-GRANULAR-ACT] More granular .act status reporting is experimental and may
      * change in the future
      */
-    onToolCallRequestStart?: (roundIndex: number, callId: number) => void;
+    onToolCallRequestStart?: (roundIndex: number, callId: number, info: {
+        /**
+         * The LLM-specific tool call ID that should go into the context. This will be the same as the
+         * `toolCallRequest.id`. Depending on the LLM, this may or may not exist, and the format of it
+         * may also vary.
+         *
+         * If you need to match up different stages of the tool call, please use the `callId`, which
+         * is provided by lmstudio.js and is guaranteed to behave consistently across all LLMs.
+         */
+        toolCallId?: string;
+    }) => void;
     /**
      * A callback that is called when the model has received the name of the tool.
      *
-     * This hook is intended for updating the UI to show the name of the tool that is being called.
-     * There is no guarantee that this callback will be called.
+     * This hook is intended for updating the UI to show the name of the tool that is being called. If
+     * the model being used does not support eager function name reporting, this callback will be
+     * called right before the `onToolCallRequestEnd` callback.
      *
      * @experimental [EXP-GRANULAR-ACT] More granular .act status reporting is experimental and may
      * change in the future
@@ -3860,7 +4159,9 @@ export declare interface LLMActBaseOpts<TPredictionResult> {
      * A callback that is called when the model has generated a fragment of the arguments of the tool.
      *
      * This hook is intended for updating the UI to stream the arguments of the tool that is being
-     * called. There is no guarantee that this callback will be called.
+     * called. If the model being used does not support function arguments streaming, this callback
+     * will be called right before the `onToolCallRequestEnd` callback, but after the
+     * `onToolCallRequestNameReceived`.
      *
      * Note, when piecing together all the argument fragments, there is no guarantee that the result
      * will be valid JSON, as some models may not use JSON to represent tool calls.
@@ -3894,6 +4195,10 @@ export declare interface LLMActBaseOpts<TPredictionResult> {
         /**
          * The tool call request that was generated by the model. This field is especially unstable
          * as we will likely replace it with a nicer type.
+         *
+         * Note, this is not guaranteed to be the actual parameters that will be passed to the tool
+         * as the `guardToolCall` handler may modify them. If you want to access the final parameters
+         * (i.e. to add to the history), you should use the `onToolCallRequestFinalized`.
          */
         toolCallRequest: ToolCallRequest;
         /**
@@ -3905,6 +4210,29 @@ export declare interface LLMActBaseOpts<TPredictionResult> {
          */
         rawContent: string | undefined;
     }) => void;
+    /**
+     * A callback that is called right before the tool call is executed. This is called after the
+     * `guardToolCall` handler (if provided) and will have the updated parameters if the
+     * `guardToolCall` updated them.
+     *
+     * @experimental [EXP-GRANULAR-ACT] More granular .act status reporting is experimental and may
+     * change in the future
+     */
+    onToolCallRequestFinalized?: (roundIndex: number, callId: number, info: {
+        /**
+         * The tool call request that is about to be executed.
+         */
+        toolCallRequest: ToolCallRequest;
+        /**
+         * The raw output that represents this tool call. It is recommended to present this to
+         * the user as is, if desired.
+         *
+         * @remarks It is not guaranteed to be valid JSON as the model does not necessarily use
+         * JSON to represent tool calls. In addition, it might not match up the `toolCallRequest`
+         * as the `guardToolCall` handler may modify the parameters.
+         */
+        rawContent: string | undefined;
+    }) => void;
     /**
      * A callback that is called when a tool call has failed to generate.
      *
@@ -3932,6 +4260,55 @@ export declare interface LLMActBaseOpts<TPredictionResult> {
      * change in the future
      */
     onToolCallRequestDequeued?: (roundIndex: number, callId: number) => void;
+    /**
+     * A handler that is called right before a tool call is executed.
+     *
+     * You may allow/allowAndOverrideParameters/deny the tool call in this handler by calling the
+     * respective method on the controller object passed in as the third parameter.
+     *
+     * An example `guardToolCll` that denies all tool calls is given below:
+     *
+     * ```ts
+     * model.act(history, tools, {
+     *   guardToolCall: (roundIndex, callId, { deny }) => {
+     *     deny("Tool calls are not allowed :(");
+     *   },
+     * });
+     * ```
+     *
+     * A more sophisticated example that prompts the user to confirm the tool call in CLI is given
+     * below (needs to be run in a Node.js environment):
+     *
+     * ```ts
+     * import readline from "readline/promises";
+     *
+     * // ...
+     *
+     * model.act(history, tools, {
+     *   guardToolCall: async (roundIndex, callId, { toolCallRequest, allow, deny }) => {
+     *     const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
+     *     const answer = await rl.question(
+     *       `Allow tool ${toolCallRequest.name}(${JSON.stringify(toolCallRequest.arguments)})? (y/n): `
+     *     );
+     *     rl.close();
+     *     if (answer.trim().toLowerCase() === "y") {
+     *       allow();
+     *     } else {
+     *       deny("Tool call denied by user.");
+     *     }
+     *   },
+     * });
+     * ```
+     *
+     * @experimental [EXP-GRANULAR-ACT] More granular .act status reporting is experimental and may
+     * change in the future
+     *
+     * @remarks
+     *
+     * You must call one of the methods on the controller object to allow or deny the tool call. If
+     * you do not call any of the methods, `.act` will fail.
+     */
+    guardToolCall?: (roundIndex: number, callId: number, controller: GuardToolCallController) => any | Promise<any>;
     /**
      * A handler that is called when a tool request is made by the model but is invalid.
      *
@@ -4267,6 +4644,11 @@ export declare type LLMGeneratorActOpts = LLMActBaseOpts<GeneratorPredictionResu
  * This may change in the future without warning.
  */
 export declare class LLMGeneratorHandle {
+    /**
+     * The identifier of the plugin that this handle is associated with.
+     */
+    readonly identifier: string;
+    private getPluginConfigSpecifier;
     /**
      * Use the generator to produce a response based on the given history.
      */
@@ -4551,11 +4933,6 @@ export declare interface LLMManualPromptTemplate {
 
 /** @public */
 export declare class LLMNamespace extends ModelNamespace<LLMLoadModelConfig, LLMInstanceInfo, LLMInfo, LLMDynamicHandle, LLM> {
-    /**
-     * @experimental [EXP-GEN-PREDICT] Using generator plugins programmatically is still in development.
-     * This may change in the future without warning.
-     */
-    createGeneratorHandle(pluginIdentifier: string): LLMGeneratorHandle;
 }
 
 /**
@@ -4909,22 +5286,35 @@ export declare interface LLMPredictionOpts<TStructuredOutputType = unknown> exte
      * @experimental [EXP-NON-ACT-TOOL-CALLBACKS] Tool call callbacks in .respond/.complete is in an
      * experimental feature. This may change in the future without warning.
      */
-    onToolCallRequestStart?: () => void;
+    onToolCallRequestStart?: (callId: number, info: {
+        /**
+         * The LLM-specific tool call ID that should go into the context. This will be the same as the
+         * `toolCallRequest.id`. Depending on the LLM, this may or may not exist, and the format of it
+         * may also vary.
+         *
+         * If you need to match up different stages of the tool call, please use the `callId`, which
+         * is provided by lmstudio.js and is guaranteed to behave consistently across all LLMs.
+         */
+        toolCallId?: string;
+    }) => void;
     /**
      * A callback that is called when the model has received the name of the tool.
      *
-     * This hook is intended for updating the UI to show the name of the tool that is being called.
-     * There is no guarantee that this callback will be called.
+     * This hook is intended for updating the UI to show the name of the tool that is being called. If
+     * the model being used does not support eager function name reporting, this callback will be
+     * called right before the `onToolCallRequestEnd` callback.
      *
      * @experimental [EXP-NON-ACT-TOOL-CALLBACKS] Tool call callbacks in .respond/.complete is in an
      * experimental feature. This may change in the future without warning.
      */
-    onToolCallRequestNameReceived?: (name: string) => void;
+    onToolCallRequestNameReceived?: (callId: number, name: string) => void;
     /**
      * A callback that is called when the model has generated a fragment of the arguments of the tool.
      *
      * This hook is intended for updating the UI to stream the arguments of the tool that is being
-     * called. There is no guarantee that this callback will be called.
+     * called. If the model being used does not support function arguments streaming, this callback
+     * will be called right before the `onToolCallRequestEnd` callback, but after the
+     * `onToolCallRequestNameReceived`.
      *
      * Note, when piecing together all the argument fragments, there is no guarantee that the result
      * will be valid JSON, as some models may not use JSON to represent tool calls.
@@ -4932,14 +5322,14 @@ export declare interface LLMPredictionOpts<TStructuredOutputType = unknown> exte
      * @experimental [EXP-NON-ACT-TOOL-CALLBACKS] Tool call callbacks in .respond/.complete is in an
      * experimental feature. This may change in the future without warning.
      */
-    onToolCallRequestArgumentFragmentGenerated?: (content: string) => void;
+    onToolCallRequestArgumentFragmentGenerated?: (callId: number, content: string) => void;
     /**
      * A callback that is called when a tool call is requested by the model.
      *
      * @experimental [EXP-NON-ACT-TOOL-CALLBACKS] Tool call callbacks in .respond/.complete is in an
      * experimental feature. This may change in the future without warning.
      */
-    onToolCallRequestEnd?: (info: {
+    onToolCallRequestEnd?: (callId: number, info: {
         /**
          * The tool call request that was generated by the model. This field is especially unstable
          * as we will likely replace it with a nicer type.
@@ -4960,7 +5350,7 @@ export declare interface LLMPredictionOpts<TStructuredOutputType = unknown> exte
      * @experimental [EXP-NON-ACT-TOOL-CALLBACKS] Tool call callbacks in .respond/.complete is in an
      * experimental feature. This may change in the future without warning.
      */
-    onToolCallRequestFailure?: (error: ToolCallRequestError) => void;
+    onToolCallRequestFailure?: (callId: number, error: ToolCallRequestError) => void;
     /**
      * An abort signal that can be used to cancel the prediction.
      */
@@ -5258,6 +5648,7 @@ export declare type LLMToolParameters = {
     properties: Record<string, any>;
     required?: string[];
     additionalProperties?: boolean;
+    $defs?: Record<string, any>;
 };
 
 /**
@@ -6141,7 +6532,7 @@ declare class PluginSelfRegistrationHost {
      */
     setPromptPreprocessor(promptPreprocessor: PromptPreprocessor): void;
     /**
-     * Sets the promptPreprocessor to be used by the plugin represented by this client.
+     * Sets the prediction loop handler to be used by the plugin represented by this client.
      *
      * @deprecated [DEP-PLUGIN-PREDICTION-LOOP-HANDLER] Prediction loop handler support is still in
      * development. Stay tuned for updates.
@@ -6207,10 +6598,57 @@ export declare class PluginsNamespace {
      * @deprecated This method is used by plugins internally to register hooks. Do not use directly.
      */
     getSelfRegistrationHost(): PluginSelfRegistrationHost;
+    /**
+     * Starts a tool use session use any config specifier.
+     */
+    private internalStartToolUseSession;
+    /**
+     * Start a tool use session with a plugin. Note, this method must be used with "Explicit Resource
+     * Management". That is, you should use it like so:
+     *
+     * ```typescript
+     * using pluginTools = await client.plugins.pluginTools("owner/name", { ... });
+     * // ^ Notice the `using` keyword here.
+     * ```
+     *
+     * If you do not use `using`, you must call `pluginTools[Symbol.dispose]()` after you are done.
+     * Otherwise, there will be a memory leak and the plugins you requested tools from will be loaded
+     * indefinitely.
+     *
+     * @experimental [EXP-USE-USE-PLUGIN-TOOLS] Using tools from other applications is still in
+     * development. This may change in the future without warning.
+     */
+    pluginTools(pluginIdentifier: string, opts?: PluginToolsOpts): Promise<RemoteToolUseSession>;
+    /**
+     * @experimental [EXP-GEN-PREDICT] Using generator plugins programmatically is still in
+     * development. This may change in the future without warning.
+     */
+    createGeneratorHandle(pluginIdentifier: string): LLMGeneratorHandle;
 }
 
 declare type PluginsPort = InferClientPort<typeof createPluginsBackendInterface>;
 
+/**
+ * Options to use with {@link PluginsNamespace#pluginTools}.
+ *
+ * @experimental [EXP-USE-USE-PLUGIN-TOOLS] Using tools from other applications is still in
+ * development. This may change in the future without warning.
+ *
+ * @public
+ */
+declare interface PluginToolsOpts {
+    /**
+     * @deprecated [DEP-PLUGIN-RAW-CONFIG] Plugin config access API is still in active development.
+     * Stay tuned for updates.
+     */
+    pluginConfig?: KVConfig;
+    /**
+     * The working directory to use for the plugin tools. If not provided, the tools provider will not
+     * get a working directory.
+     */
+    workingDirectory?: string;
+}
+
 /**
  * TODO: Documentation
  *
@@ -6398,6 +6836,7 @@ export declare class PredictionResult implements BasePredictionResult {
  * @public
  */
 export declare class ProcessingController extends BaseController {
+    private readonly enabledPluginInfos;
     private sendUpdate;
     /**
      * Gets a mutable copy of the current history. The returned history is a copy, so mutating it will
@@ -6415,10 +6854,17 @@ export declare class ProcessingController extends BaseController {
     createCitationBlock(citedText: string, source: CreateCitationBlockOpts): PredictionProcessCitationBlockController;
     createContentBlock({ roleOverride, includeInContext, style, prefix, suffix, }?: CreateContentBlockOpts): PredictionProcessContentBlockController;
     debug(...messages: Array<any>): void;
-    getPredictionConfig(): LLMPredictionConfig;
-    readonly model: Readonly<{
-        getOrLoad: () => Promise<LLM>;
-    }>;
+    /**
+     * Gets the token source associated with this prediction process (i.e. what the user has selected
+     * on the top navigation bar).
+     *
+     * The token source can either be a model or a generator plugin. In both cases, the returned
+     * object will contain a ".act" and a ".respond" method, which can be used to generate text.
+     *
+     * The token source is already pre-configured to use user's prediction config - you don't need to
+     * pass through any additional configuration.
+     */
+    tokenSource(): Promise<LLM | LLMGeneratorHandle>;
     /**
      * Sets the sender name for this message. The sender name shown above the message in the chat.
      */
@@ -6442,6 +6888,24 @@ export declare class ProcessingController extends BaseController {
     suggestName(name: string): Promise<void>;
     requestConfirmToolCall({ callId, pluginIdentifier, name, parameters, }: RequestConfirmToolCallOpts): Promise<RequestConfirmToolCallResult>;
     createToolStatus(callId: number, initialStatus: ToolStatusStepStateStatus): PredictionProcessToolStatusController;
+    /**
+     * Starts a tool use session with tools available in the prediction process. Note, this method
+     * should be used with "Explicit Resource Management". That is, you should use it like so:
+     *
+     * ```typescript
+     * using toolUseSession = await ctl.startToolUseSession();
+     * // ^ Notice the `using` keyword here.
+     * ```
+     *
+     * If you do not `using`, you should call `toolUseSession[Symbol.dispose]()` after you are done.
+     *
+     * If you don't, lmstudio-js will close the session upon the end of the prediction step
+     * automatically. However, it is not recommended.
+     *
+     * @public
+     * @deprecated WIP
+     */
+    startToolUseSession(): Promise<RemoteToolUseSession>;
 }
 
 declare type ProcessingRequest = ProcessingRequestConfirmToolCall | ProcessingRequestTextInput;
@@ -6769,6 +7233,77 @@ export declare interface RegisterDevelopmentPluginResult {
     unregister: () => Promise<void>;
 }
 
+/**
+ * Represents a plugin that is currently available in LM Studio.
+ *
+ * @experimental [EXP-USE-PLUGINS-API] Using plugins API is still in development. This may change in
+ * the future without warning.
+ *
+ * @public
+ */
+declare interface RemotePluginInfo {
+    /**
+     * The identifier of the plugin. For non-dev plugins, this is the same as the artifact identifier
+     * when uploaded to LM Studio Hub. For example, `lmstudio/dice`.
+     *
+     * For dev plugins, this will be prefixed with `dev/` to indicate that it is a development
+     * version. For example, `dev/owner/plugin-name`.
+     *
+     * The exact format of this identifier may change in the future. You should not parse it.
+     */
+    identifier: string;
+    /**
+     * Whether this plugin is in development mode, e.g. running externally using `lms dev`.
+     */
+    isDev: boolean;
+    /**
+     * Whether this plugin is trusted.
+     */
+    isTrusted: boolean;
+    /**
+     * Whether this plugin has a prompt preprocessor component.
+     */
+    hasPromptPreprocessor: boolean;
+    /**
+     * Whether this plugin has a prediction loop handler component.
+     */
+    hasPredictionLoopHandler: boolean;
+    /**
+     * Whether this plugin has a tools provider component.
+     */
+    hasToolsProvider: boolean;
+    /**
+     * Whether this plugin has a generator component.
+     */
+    hasGenerator: boolean;
+}
+
+/**
+ * Represents a tool that is exposed by LMStudio plugins.
+ *
+ * @public
+ * @experimental [EXP-USE-USE-PLUGIN-TOOLS] Using tools from other plugins is still in development.
+ * This may change in the future without warning.
+ */
+declare interface RemoteTool extends ToolBase {
+    type: "remoteTool";
+    /**
+     * Which plugin this tool belongs to.
+     */
+    pluginIdentifier: string;
+    parametersJsonSchema: any;
+    checkParameters: (params: any) => void;
+    implementation: (params: Record<string, unknown>, ctx: ToolCallContext) => any | Promise<any>;
+}
+
+/**
+ * Represents a session for using remote tools.
+ */
+declare interface RemoteToolUseSession extends Disposable {
+    tools: Array<RemoteTool>;
+    [Symbol.dispose](): void;
+}
+
 /** @public */
 export declare class RepositoryNamespace {
     private readonly repositoryPort;
@@ -7797,6 +8332,7 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
     }>;
 }, "strip", z.ZodTypeAny, {
     type: "rpcError";
+    callId: number;
     error: {
         title: string;
         cause?: string | undefined;
@@ -7848,9 +8384,9 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
         stack?: string | undefined;
         rootTitle?: string | undefined;
     };
-    callId: number;
 }, {
     type: "rpcError";
+    callId: number;
     error: {
         title?: string | undefined;
         cause?: string | undefined;
@@ -7902,7 +8438,6 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
         stack?: string | undefined;
         rootTitle?: string | undefined;
     };
-    callId: number;
 }>, z.ZodObject<{
     type: z.ZodLiteral<"signalUpdate">;
     subscribeId: z.ZodNumber;
@@ -8116,6 +8651,7 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
     }>;
 }, "strip", z.ZodTypeAny, {
     type: "signalError";
+    subscribeId: number;
     error: {
         title: string;
         cause?: string | undefined;
@@ -8167,9 +8703,9 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
         stack?: string | undefined;
         rootTitle?: string | undefined;
     };
-    subscribeId: number;
 }, {
     type: "signalError";
+    subscribeId: number;
     error: {
         title?: string | undefined;
         cause?: string | undefined;
@@ -8221,7 +8757,6 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
         stack?: string | undefined;
         rootTitle?: string | undefined;
     };
-    subscribeId: number;
 }>, z.ZodObject<{
     type: z.ZodLiteral<"writableSignalUpdate">;
     subscribeId: z.ZodNumber;
@@ -8435,6 +8970,7 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
     }>;
 }, "strip", z.ZodTypeAny, {
     type: "writableSignalError";
+    subscribeId: number;
     error: {
         title: string;
         cause?: string | undefined;
@@ -8486,9 +9022,9 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
         stack?: string | undefined;
         rootTitle?: string | undefined;
     };
-    subscribeId: number;
 }, {
     type: "writableSignalError";
+    subscribeId: number;
     error: {
         title?: string | undefined;
         cause?: string | undefined;
@@ -8540,7 +9076,6 @@ declare const serverToClientMessageSchema: z.ZodDiscriminatedUnion<"type", [z.Zo
         stack?: string | undefined;
         rootTitle?: string | undefined;
     };
-    subscribeId: number;
 }>]>;
 
 /**
@@ -8881,12 +9416,20 @@ export declare function text(strings: TemplateStringsArray, ...values: ReadonlyA
  */
 export declare type TextAllowedTypes = string | number | object;
 
+declare type TokenSourceIdentifier = {
+    type: "model";
+    identifier: string;
+} | {
+    type: "generator";
+    pluginIdentifier: string;
+};
+
 /**
  * Represents a tool that can be given to an LLM with `.act`.
  *
  * @public
  */
-export declare type Tool = FunctionTool | RawFunctionTool;
+export declare type Tool = FunctionTool | RawFunctionTool | UnimplementedRawFunctionTool | RemoteTool;
 
 /**
  * A function that can be used to create a function `Tool` given a function definition and its
@@ -9113,6 +9656,26 @@ declare abstract class Transport<TIncoming, TOutgoing> {
     [Symbol.asyncDispose](): Promise<void>;
 }
 
+declare interface UnimplementedRawFunctionTool extends ToolBase {
+    type: "unimplementedRawFunction";
+    parametersJsonSchema: any;
+    checkParameters: (params: any) => void;
+    implementation: (params: Record<string, unknown>, ctx: ToolCallContext) => never;
+}
+
+/**
+ * A function that can be used to create a raw function `Tool` that is not implemented yet. When
+ * using `.act`, upon encountering an unimplemented tool, the `.act` will stop gracefully.
+ *
+ * @public
+ * @experimental Not stable, will likely change in the future.
+ */
+export declare function unimplementedRawFunctionTool({ name, description, parametersJsonSchema, }: {
+    name: string;
+    description: string;
+    parametersJsonSchema: any;
+}): UnimplementedRawFunctionTool;
+
 declare class Validator {
     private readonly attachStack;
     constructor({ attachStack }?: ValidatorConstructorOpts);
diff --git a/node_modules/@lmstudio/sdk/dist/index.mjs b/node_modules/@lmstudio/sdk/dist/index.mjs
index f83a2d0..fec1a8c 100644
--- a/node_modules/@lmstudio/sdk/dist/index.mjs
+++ b/node_modules/@lmstudio/sdk/dist/index.mjs
@@ -1,9 +1,12 @@
 import { z } from 'zod';
-import process$1 from 'process';
 import chalk from 'chalk';
 import { terminalSize, WebSocket, readFileAsBase64, generateRandomBase64 } from '@lmstudio/lms-isomorphic';
 import { zodToJsonSchema } from 'zod-to-json-schema';
 
+if (typeof process == 'undefined') {
+    process = {}
+}
+
 function isSignalLike(value) {
     return (typeof value === "object" &&
         value !== null &&
@@ -2040,6 +2043,7 @@ class Cleaner {
     }
 }
 
+// import process from "process";
 // Error stack manipulation related functions
 function getCurrentStack(goAbove = 0) {
     const stack = new Error().stack;
@@ -2050,7 +2054,7 @@ function getCurrentStack(goAbove = 0) {
     return lines.slice(2 + goAbove).join("\n");
 }
 function changeErrorStackInPlace(error, newStack) {
-    if (process$1.env.LMS_KEEP_INTERNAL_STACK) {
+    if (process.env.LMS_KEEP_INTERNAL_STACK) {
         return;
     }
     const stackContent = error.stack ?? "";
@@ -2059,6 +2063,17 @@ function changeErrorStackInPlace(error, newStack) {
         newStack).trimEnd();
 }
 
+class IdGiver {
+    constructor(firstId = 0) {
+        this.nextId = firstId;
+    }
+    next() {
+        const id = this.nextId;
+        this.nextId++;
+        return id;
+    }
+}
+
 function getDefaultExportFromCjs (x) {
 	return x && x.__esModule && Object.prototype.hasOwnProperty.call(x, 'default') ? x['default'] : x;
 }
@@ -4419,7 +4434,7 @@ function makeTitledPrettyError(title, content, stack) {
     return makePrettyError(chalk.redBright(title) + "\n\n" + content, stack);
 }
 function makePrettyError(content, stack) {
-    if (process$1.browser || process$1.env.LMS_NO_FANCY_ERRORS || terminalSize().columns < 80) {
+    if (process.browser || process.env.LMS_NO_FANCY_ERRORS || terminalSize().columns < 80) {
         const error = new Error(content);
         if (stack === undefined) {
             changeErrorStackInPlace(error, "");
@@ -5085,6 +5100,10 @@ const artifactManifestBaseSchema = z.object({
     dependencies: z.array(artifactDependencySchema).optional(),
     tags: z.array(z.string()).optional(),
 });
+const artifactIdentifierRegex = /^[a-z0-9]+(?:-[a-z0-9]+)*\/[a-z0-9]+(?:[-.][a-z0-9]+)*$/;
+const artifactIdentifierSchema = z.string().regex(artifactIdentifierRegex, {
+    message: "Invalid artifact identifier format. Expected 'owner/name'.",
+});
 
 const modelManifestSchema = z.object({
     type: z.literal("model"),
@@ -5631,6 +5650,7 @@ const llmToolParametersSchema = z.discriminatedUnion("type", [
         properties: z.record(jsonSerializableSchema),
         required: z.array(z.string()).optional(),
         additionalProperties: z.boolean().optional(),
+        $defs: z.record(jsonSerializableSchema).optional(),
     }),
     // add more parameter types here
     // ...
@@ -6113,6 +6133,17 @@ z.discriminatedUnion("type", [
     processingUpdateDebugInfoBlockCreateSchema,
 ]);
 
+const tokenSourceIdentifierSchema = z.discriminatedUnion("type", [
+    z.object({
+        type: z.literal("model"),
+        identifier: z.string(),
+    }),
+    z.object({
+        type: z.literal("generator"),
+        pluginIdentifier: z.string(),
+    }),
+]);
+
 const modelInfoSchema = z.discriminatedUnion("type", [
     llmInfoSchema,
     embeddingModelInfoSchema,
@@ -6122,6 +6153,29 @@ const modelInstanceInfoSchema = z.discriminatedUnion("type", [
     embeddingModelInstanceInfoSchema,
 ]);
 
+const pluginConfigSpecifierSchema = z.discriminatedUnion("type", [
+    z.object({
+        type: z.literal("direct"),
+        config: kvConfigSchema,
+        workingDirectoryPath: z.string().optional(),
+    }),
+    z.object({
+        type: z.literal("predictionProcess"),
+        pci: z.string(),
+        token: z.string(),
+    }),
+]);
+
+const remotePluginInfoSchema = z.object({
+    identifier: z.string(),
+    isDev: z.boolean(),
+    isTrusted: z.boolean(),
+    hasPromptPreprocessor: z.boolean(),
+    hasPredictionLoopHandler: z.boolean(),
+    hasToolsProvider: z.boolean(),
+    hasGenerator: z.boolean(),
+});
+
 const artifactDownloadPlanModelInfoSchema = z.object({
     displayName: z.string(),
     sizeBytes: z.number(),
@@ -9317,98 +9371,6 @@ function maybeFalseNumberToCheckboxNumeric(maybeFalseNumber, valueWhenUnchecked)
     return { checked: true, value: maybeFalseNumber };
 }
 
-function kvConfigToLLMPredictionConfig(config) {
-    const result = {};
-    const parsed = globalConfigSchematics.parsePartial(config);
-    const maxPredictedTokens = parsed.get("llm.prediction.maxPredictedTokens");
-    if (maxPredictedTokens !== undefined) {
-        result.maxTokens = maxPredictedTokens.checked ? maxPredictedTokens.value : false;
-    }
-    const temperature = parsed.get("llm.prediction.temperature");
-    if (temperature !== undefined) {
-        result.temperature = temperature;
-    }
-    const stopStrings = parsed.get("llm.prediction.stopStrings");
-    if (stopStrings !== undefined) {
-        result.stopStrings = stopStrings;
-    }
-    const toolCallStopStrings = parsed.get("llm.prediction.toolCallStopStrings");
-    if (toolCallStopStrings !== undefined) {
-        result.toolCallStopStrings = toolCallStopStrings;
-    }
-    const contextOverflowPolicy = parsed.get("llm.prediction.contextOverflowPolicy");
-    if (contextOverflowPolicy !== undefined) {
-        result.contextOverflowPolicy = contextOverflowPolicy;
-    }
-    const structured = parsed.get("llm.prediction.structured");
-    if (structured !== undefined) {
-        result.structured = structured;
-    }
-    const tools = parsed.get("llm.prediction.tools");
-    if (tools !== undefined) {
-        result.rawTools = tools;
-    }
-    const topKSampling = parsed.get("llm.prediction.topKSampling");
-    if (topKSampling !== undefined) {
-        result.topKSampling = topKSampling;
-    }
-    const repeatPenalty = parsed.get("llm.prediction.repeatPenalty");
-    if (repeatPenalty !== undefined) {
-        result.repeatPenalty = repeatPenalty.checked ? repeatPenalty.value : false;
-    }
-    const minPSampling = parsed.get("llm.prediction.minPSampling");
-    if (minPSampling !== undefined) {
-        result.minPSampling = minPSampling.checked ? minPSampling.value : false;
-    }
-    const topPSampling = parsed.get("llm.prediction.topPSampling");
-    if (topPSampling !== undefined) {
-        result.topPSampling = topPSampling.checked ? topPSampling.value : false;
-    }
-    const xtcProbability = parsed.get("llm.prediction.llama.xtcProbability");
-    if (xtcProbability !== undefined) {
-        result.xtcProbability = xtcProbability.checked ? xtcProbability.value : false;
-    }
-    const xtcThreshold = parsed.get("llm.prediction.llama.xtcThreshold");
-    if (xtcThreshold !== undefined) {
-        result.xtcThreshold = xtcThreshold.checked ? xtcThreshold.value : false;
-    }
-    const logProbs = parsed.get("llm.prediction.logProbs");
-    if (logProbs !== undefined) {
-        result.logProbs = logProbs.checked ? logProbs.value : false;
-    }
-    const cpuThreads = parsed.get("llm.prediction.llama.cpuThreads");
-    if (cpuThreads !== undefined) {
-        result.cpuThreads = cpuThreads;
-    }
-    const promptTemplate = parsed.get("llm.prediction.promptTemplate");
-    if (promptTemplate !== undefined) {
-        result.promptTemplate = promptTemplate;
-    }
-    const speculativeDecodingDraftModel = parsed.get("llm.prediction.speculativeDecoding.draftModel");
-    if (speculativeDecodingDraftModel !== undefined) {
-        result.draftModel = speculativeDecodingDraftModel;
-    }
-    const speculativeDecodingDraftTokensExact = parsed.get("llm.prediction.speculativeDecoding.numDraftTokensExact");
-    if (speculativeDecodingDraftTokensExact !== undefined) {
-        result.speculativeDecodingNumDraftTokensExact = speculativeDecodingDraftTokensExact;
-    }
-    const speculativeDecodingMinContinueDraftingProbability = parsed.get("llm.prediction.speculativeDecoding.minContinueDraftingProbability");
-    if (speculativeDecodingMinContinueDraftingProbability !== undefined) {
-        result.speculativeDecodingMinContinueDraftingProbability =
-            speculativeDecodingMinContinueDraftingProbability;
-    }
-    const speculativeDecodingMinDraftLengthToConsider = parsed.get("llm.prediction.speculativeDecoding.minDraftLengthToConsider");
-    if (speculativeDecodingMinDraftLengthToConsider !== undefined) {
-        result.speculativeDecodingMinDraftLengthToConsider =
-            speculativeDecodingMinDraftLengthToConsider;
-    }
-    const reasoningParsing = parsed.get("llm.prediction.reasoning.parsing");
-    if (reasoningParsing !== undefined) {
-        result.reasoningParsing = reasoningParsing;
-    }
-    result.raw = config;
-    return result;
-}
 function llmPredictionConfigToKVConfig(config) {
     const top = llmPredictionConfigSchematics.buildPartialConfig({
         "temperature": config.temperature,
@@ -11319,14 +11281,34 @@ class SimpleToolCallContext {
 const functionToolSchema = toolBaseSchema.extend({
     type: z.literal("function"),
     parametersSchema: zodSchemaSchema,
+    checkParameters: z.function(),
     implementation: z.function(),
 });
 const rawFunctionToolSchema = toolBaseSchema.extend({
     type: z.literal("rawFunction"),
     parametersSchema: zodSchemaSchema,
+    checkParameters: z.function(),
+    implementation: z.function(),
+});
+const unimplementedRawFunctionToolSchema = toolBaseSchema.extend({
+    type: z.literal("unimplementedRawFunction"),
+    parametersJsonSchema: zodSchemaSchema,
+    checkParameters: z.function(),
+    implementation: z.function(),
+});
+const remoteToolSchema = toolBaseSchema.extend({
+    type: z.literal("remoteTool"),
+    pluginIdentifier: z.string(),
+    parametersJsonSchema: zodSchemaSchema,
+    checkParameters: z.function(),
     implementation: z.function(),
 });
-z.discriminatedUnion("type", [functionToolSchema, rawFunctionToolSchema]);
+z.discriminatedUnion("type", [
+    functionToolSchema,
+    rawFunctionToolSchema,
+    unimplementedRawFunctionToolSchema,
+    remoteToolSchema,
+]);
 /**
  * A function that can be used to create a function `Tool` given a function definition and its
  * implementation.
@@ -11395,6 +11377,63 @@ function rawFunctionTool({ name, description, parametersJsonSchema, implementati
         implementation,
     };
 }
+class UnimplementedToolError extends Error {
+    constructor(toolName) {
+        super(`Tool "${toolName}" is not implemented.`);
+    }
+}
+/**
+ * A function that can be used to create a raw function `Tool` that is not implemented yet. When
+ * using `.act`, upon encountering an unimplemented tool, the `.act` will stop gracefully.
+ *
+ * @public
+ * @experimental Not stable, will likely change in the future.
+ */
+function unimplementedRawFunctionTool({ name, description, parametersJsonSchema, }) {
+    const jsonSchemaValidator = new libExports.Validator();
+    return {
+        name,
+        description,
+        type: "unimplementedRawFunction",
+        parametersJsonSchema,
+        checkParameters(params) {
+            const validationResult = jsonSchemaValidator.validate(params, parametersJsonSchema);
+            if (validationResult.errors.length > 0) {
+                throw new Error(text `
+          Failed to parse arguments for tool "${name}":
+          ${jsonSchemaValidationErrorToAIReadableText("params", validationResult.errors)}
+        `);
+            }
+        },
+        implementation: () => {
+            throw new UnimplementedToolError(name);
+        },
+    };
+}
+/**
+ * Creates a tool that represents a remote tool exposed by an LMStudio plugin. This function is not
+ * exposed and is used internally by the plugins namespace.
+ */
+function internalCreateRemoteTool({ name, description, pluginIdentifier, parametersJsonSchema, implementation, }) {
+    return {
+        name,
+        description,
+        type: "remoteTool",
+        pluginIdentifier,
+        parametersJsonSchema,
+        checkParameters: params => {
+            const jsonSchemaValidator = new libExports.Validator();
+            const validationResult = jsonSchemaValidator.validate(params, parametersJsonSchema);
+            if (validationResult.errors.length > 0) {
+                throw new Error(text `
+          Failed to parse arguments for tool "${name}":
+          ${jsonSchemaValidationErrorToAIReadableText("params", validationResult.errors)}
+        `);
+            }
+        },
+        implementation,
+    };
+}
 function functionToolToLLMTool(tool) {
     return {
         type: "function",
@@ -11415,6 +11454,16 @@ function rawFunctionToolToLLMTool(tool) {
         },
     };
 }
+function remoteToolToLLMTool(tool) {
+    return {
+        type: "function",
+        function: {
+            name: tool.name,
+            description: tool.description,
+            parameters: tool.parametersJsonSchema,
+        },
+    };
+}
 /**
  * Convert a `Tool` to a internal `LLMTool`.
  */
@@ -11424,7 +11473,10 @@ function toolToLLMTool(tool) {
         case "function":
             return functionToolToLLMTool(tool);
         case "rawFunction":
+        case "unimplementedRawFunction":
             return rawFunctionToolToLLMTool(tool);
+        case "remoteTool":
+            return remoteToolToLLMTool(tool);
         default: {
             const exhaustiveCheck = type;
             throw new Error(`Unhandled type: ${exhaustiveCheck}`);
@@ -13993,6 +14045,10 @@ function createLlmBackendInterface() {
             }),
             z.object({
                 type: z.literal("toolCallGenerationStart"),
+                /**
+                 * The LLM specific call id of the tool call.
+                 */
+                toolCallId: z.string().optional(),
             }),
             z.object({
                 type: z.literal("toolCallGenerationNameReceived"),
@@ -14039,13 +14095,134 @@ function createLlmBackendInterface() {
                 type: z.literal("cancel"),
             }),
         ]),
+    })
+        .addRpcEndpoint("applyPromptTemplate", {
+        parameter: z.object({
+            specifier: modelSpecifierSchema,
+            history: chatHistoryDataSchema,
+            predictionConfigStack: kvConfigStackSchema,
+            opts: llmApplyPromptTemplateOptsSchema,
+        }),
+        returns: z.object({
+            formatted: z.string(),
+        }),
+    })
+        .addRpcEndpoint("tokenize", {
+        parameter: z.object({
+            specifier: modelSpecifierSchema,
+            inputString: z.string(),
+        }),
+        returns: z.object({
+            tokens: z.array(z.number()),
+        }),
+    })
+        .addRpcEndpoint("countTokens", {
+        parameter: z.object({
+            specifier: modelSpecifierSchema,
+            inputString: z.string(),
+        }),
+        returns: z.object({
+            tokenCount: z.number().int(),
+        }),
+    })
+        // Starts to eagerly preload a draft model. This is useful when you want a draft model to be
+        // ready for speculative decoding.
+        .addRpcEndpoint("preloadDraftModel", {
+        parameter: z.object({
+            specifier: modelSpecifierSchema,
+            draftModelKey: z.string(),
+        }),
+        returns: z.void(),
+    }));
+}
+
+function createPluginsBackendInterface() {
+    return (new BackendInterface()
+        /**
+         * The following method is called by a client that wants to use plugins that are registered
+         * to LM Studio.
+         */
+        .addChannelEndpoint("startToolUseSession", {
+        creationParameter: z.object({
+            pluginIdentifier: z.string(),
+            pluginConfigSpecifier: pluginConfigSpecifierSchema,
+        }),
+        toClientPacket: z.discriminatedUnion("type", [
+            /**
+             * The session has been started successfully. The client can now use the session. Note,
+             * there are no sessionError message because if a session fails to start, the channel
+             * will error instead.
+             */
+            z.object({
+                type: z.literal("sessionReady"),
+                toolDefinitions: z.array(llmToolSchema),
+            }),
+            /**
+             * A tool call has been completed.
+             */
+            z.object({
+                type: z.literal("toolCallComplete"),
+                callId: z.number(),
+                result: jsonSerializableSchema,
+            }),
+            /**
+             * A tool call has failed.
+             */
+            z.object({
+                type: z.literal("toolCallError"),
+                callId: z.number(),
+                error: serializedLMSExtendedErrorSchema,
+            }),
+            /**
+             * Status update for a tool call.
+             */
+            z.object({
+                type: z.literal("toolCallStatus"),
+                callId: z.number(),
+                statusText: z.string(),
+            }),
+            /**
+             * Warning message for a tool call.
+             */
+            z.object({
+                type: z.literal("toolCallWarn"),
+                callId: z.number(),
+                warnText: z.string(),
+            }),
+        ]),
+        toServerPacket: z.discriminatedUnion("type", [
+            /**
+             * Request to start a tool call. This call can be aborted using the `abortToolCall`
+             * packet. When the tool call is completed, either the `toolCallResult` or `toolCallError`
+             * packet will be sent.
+             */
+            z.object({
+                type: z.literal("callTool"),
+                callId: z.number(),
+                name: z.string(),
+                arguments: jsonSerializableSchema,
+            }),
+            /**
+             * Request to abort a tool call. Upon calling this, no toolCallComplete or toolCallError
+             * packets will be sent for the call. We assume abort is done immediately.
+             */
+            z.object({
+                type: z.literal("abortToolCall"),
+                callId: z.number(),
+            }),
+            /**
+             * Client requests to discard the session. Upon calling this, the channel will be closed.
+             */
+            z.object({
+                type: z.literal("discardSession"),
+            }),
+        ]),
     })
         .addChannelEndpoint("generateWithGenerator", {
         creationParameter: z.object({
             pluginIdentifier: z.string(),
-            pluginConfigStack: kvConfigStackSchema,
+            pluginConfigSpecifier: pluginConfigSpecifierSchema,
             tools: z.array(llmToolSchema),
-            workingDirectoryPath: z.string().nullable(),
             history: chatHistoryDataSchema,
         }),
         toClientPacket: z.discriminatedUnion("type", [
@@ -14059,6 +14236,10 @@ function createLlmBackendInterface() {
             }),
             z.object({
                 type: z.literal("toolCallGenerationStart"),
+                /**
+                 * The LLM specific call id of the tool call.
+                 */
+                toolCallId: z.string().optional(),
             }),
             z.object({
                 type: z.literal("toolCallGenerationNameReceived"),
@@ -14085,48 +14266,6 @@ function createLlmBackendInterface() {
             }),
         ]),
     })
-        .addRpcEndpoint("applyPromptTemplate", {
-        parameter: z.object({
-            specifier: modelSpecifierSchema,
-            history: chatHistoryDataSchema,
-            predictionConfigStack: kvConfigStackSchema,
-            opts: llmApplyPromptTemplateOptsSchema,
-        }),
-        returns: z.object({
-            formatted: z.string(),
-        }),
-    })
-        .addRpcEndpoint("tokenize", {
-        parameter: z.object({
-            specifier: modelSpecifierSchema,
-            inputString: z.string(),
-        }),
-        returns: z.object({
-            tokens: z.array(z.number()),
-        }),
-    })
-        .addRpcEndpoint("countTokens", {
-        parameter: z.object({
-            specifier: modelSpecifierSchema,
-            inputString: z.string(),
-        }),
-        returns: z.object({
-            tokenCount: z.number().int(),
-        }),
-    })
-        // Starts to eagerly preload a draft model. This is useful when you want a draft model to be
-        // ready for speculative decoding.
-        .addRpcEndpoint("preloadDraftModel", {
-        parameter: z.object({
-            specifier: modelSpecifierSchema,
-            draftModelKey: z.string(),
-        }),
-        returns: z.void(),
-    }));
-}
-
-function createPluginsBackendInterface() {
-    return (new BackendInterface()
         /**
          * The following method is called by the controlling client. (e.g. lms-cli)
          */
@@ -14165,6 +14304,10 @@ function createPluginsBackendInterface() {
                 pluginConfig: kvConfigSchema,
                 globalPluginConfig: kvConfigSchema,
                 workingDirectoryPath: z.string().nullable(),
+                /**
+                 * An array of all the plugins that are enabled for this prediction.
+                 */
+                enabledPluginInfos: z.array(remotePluginInfoSchema),
                 /** Processing Context Identifier */
                 pci: z.string(),
                 token: z.string(),
@@ -14201,6 +14344,10 @@ function createPluginsBackendInterface() {
                 pluginConfig: kvConfigSchema,
                 globalPluginConfig: kvConfigSchema,
                 workingDirectoryPath: z.string().nullable(),
+                /**
+                 * An array of all the plugins that are enabled for this prediction.
+                 */
+                enabledPluginInfos: z.array(remotePluginInfoSchema),
                 /** Processing Context Identifier */
                 pci: z.string(),
                 token: z.string(),
@@ -14351,6 +14498,7 @@ function createPluginsBackendInterface() {
             z.object({
                 type: z.literal("toolCallGenerationStarted"),
                 taskId: z.string(),
+                toolCallId: z.string().optional(),
             }),
             z.object({
                 type: z.literal("toolCallGenerationNameReceived"),
@@ -14403,14 +14551,14 @@ function createPluginsBackendInterface() {
         }),
         returns: chatHistoryDataSchema,
     })
-        .addRpcEndpoint("processingGetOrLoadModel", {
+        .addRpcEndpoint("processingGetOrLoadTokenSource", {
         parameter: z.object({
             /** Processing Context Identifier */
             pci: z.string(),
             token: z.string(),
         }),
         returns: z.object({
-            identifier: z.string(),
+            tokenSourceIdentifier: tokenSourceIdentifierSchema,
         }),
     })
         .addRpcEndpoint("processingHasStatus", {
@@ -15825,6 +15973,11 @@ class ActResult {
     }
 }
 
+/**
+ * Each call uses a globally unique call ID that starts somewhere before the half of the
+ * `Number.MAX_SAFE_INTEGER`.
+ */
+const callIdGiver = new IdGiver(Math.floor(Math.random() * (Number.MAX_SAFE_INTEGER / 2 / 10000)) * 10000);
 class NoQueueQueue {
     needsQueueing() {
         return false;
@@ -15956,9 +16109,58 @@ class FIFOQueue {
         this.queue = [];
     }
 }
-const llmActBaseOptsSchema = z.object({
-    onFirstToken: z.function().optional(),
-    onPredictionFragment: z.function().optional(),
+/**
+ * Controller object used to allow/modify/deny a tool call.
+ */
+class GuardToolCallController {
+    /**
+     * Don't construct this object yourself.
+     */
+    constructor(toolCallRequest, tool, resultContainer) {
+        this.toolCallRequest = toolCallRequest;
+        this.tool = tool;
+        this.resultContainer = resultContainer;
+        /**
+         * Allows the tool call to proceed without any modifications.
+         */
+        this.allow = () => {
+            this.assertNoResultYet("allow", getCurrentStack(1));
+            this.resultContainer[0] = { type: "allow" };
+        };
+        /**
+         * Allows the tool call to proceed, but overrides the parameters with the provided ones.
+         */
+        this.allowAndOverrideParameters = (newParameters) => {
+            this.assertNoResultYet("allowAndOverrideParameters", getCurrentStack(1));
+            this.resultContainer[0] = { type: "allowAndOverrideParameters", parameters: newParameters };
+        };
+        /**
+         * Denys the tool call with a specified reason. This will not interrupt the `.act` call. Instead,
+         * the reason you provide will be provided to the model as the tool call result.
+         *
+         * If `reason` is not provided, a generic default reason will be used.
+         *
+         * If you wish to immediately fail the `.act` call, you can throw an error instead.
+         */
+        this.deny = (reason) => {
+            this.assertNoResultYet("deny", getCurrentStack(1));
+            this.resultContainer[0] = { type: "deny", reason };
+        };
+    }
+    assertNoResultYet(calledMethodName, stack) {
+        if (this.resultContainer[0] === null) {
+            return;
+        }
+        // Oh no, the result has already been set! Make an error message.
+        throw makeTitledPrettyError(`Cannot call ${calledMethodName} after a result has been set`, text `
+        This tool call guard has already set a result previously (${this.resultContainer[0].type}). 
+        You cannot set a result more than once.
+      `, stack);
+    }
+}
+const llmActBaseOptsSchema = z.object({
+    onFirstToken: z.function().optional(),
+    onPredictionFragment: z.function().optional(),
     onMessage: z.function().optional(),
     onRoundStart: z.function().optional(),
     onRoundEnd: z.function().optional(),
@@ -15968,8 +16170,10 @@ const llmActBaseOptsSchema = z.object({
     onToolCallRequestNameReceived: z.function().optional(),
     onToolCallRequestArgumentFragmentGenerated: z.function().optional(),
     onToolCallRequestEnd: z.function().optional(),
+    onToolCallRequestFinalized: z.function().optional(),
     onToolCallRequestFailure: z.function().optional(),
     onToolCallRequestDequeued: z.function().optional(),
+    guardToolCall: z.function().optional(),
     handleInvalidToolRequest: z.function().optional(),
     maxPredictionRounds: z.number().int().min(1).optional(),
     signal: z.instanceof(AbortSignal).optional(),
@@ -16009,10 +16213,12 @@ const defaultHandleInvalidToolRequest = (error, request) => {
 async function internalAct(chat, tools, baseOpts, stack, logger, startTime, predictImpl, makePredictionResult) {
     const abortController = new AbortController();
     const mutableChat = Chat.from(chat); // Make a copy
+    let currentCallId = -1;
     /**
-     * Our ID that allows users to match up calls.
+     * A flag that will be set if any unimplemented tool is called. In which case, the loop will
+     * terminate after all the parallel tool calls are resolved.
      */
-    let currentCallId = 0;
+    let hasCalledUnimplementedTool = false;
     if (baseOpts.signal !== undefined) {
         if (baseOpts.signal.aborted) {
             // If the signal is already aborted, we should not continue.
@@ -16131,6 +16337,8 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
         const queue = baseOpts.allowParallelToolExecution
             ? new NoQueueQueue()
             : new FIFOQueue();
+        let receivedEagerToolNameReporting = false;
+        let receivedToolArgumentsStreaming = false;
         predictImpl({
             allowTools,
             history: accessMaybeMutableInternals(mutableChat)._internalGetData(),
@@ -16154,27 +16362,43 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
             handlePromptProcessingProgress: progress => {
                 safeCallCallback(logger, "onPromptProcessingProgress", baseOpts.onPromptProcessingProgress, [predictionsPerformed, progress]);
             },
-            handleToolCallGenerationStart: () => {
-                currentCallId++;
+            handleToolCallGenerationStart: toolCallId => {
+                currentCallId = callIdGiver.next();
+                receivedEagerToolNameReporting = false;
+                receivedToolArgumentsStreaming = false;
                 isGeneratingToolCall = true;
                 safeCallCallback(logger, "onToolCallRequestStart", baseOpts.onToolCallRequestStart, [
                     predictionsPerformed,
                     currentCallId,
+                    { toolCallId: toolCallId },
                 ]);
             },
             handleToolCallGenerationNameReceived: name => {
+                receivedEagerToolNameReporting = true;
                 safeCallCallback(logger, "onToolCallRequestNameReceived", baseOpts.onToolCallRequestNameReceived, [predictionsPerformed, currentCallId, name]);
             },
             handleToolCallGenerationArgumentFragmentGenerated: content => {
+                receivedToolArgumentsStreaming = true;
                 safeCallCallback(logger, "onToolCallRequestArgumentFragmentGenerated", baseOpts.onToolCallRequestArgumentFragmentGenerated, [predictionsPerformed, currentCallId, content]);
             },
             handleToolCallGenerationEnd: (request, rawContent) => {
+                const callId = currentCallId;
                 isGeneratingToolCall = false;
                 const toolCallIndex = nextToolCallIndex;
                 nextToolCallIndex++;
+                if (!receivedEagerToolNameReporting) {
+                    // If eager name reporting not received, report it.
+                    safeCallCallback(logger, "onToolCallRequestNameReceived", baseOpts.onToolCallRequestNameReceived, [predictionsPerformed, callId, request.name]);
+                }
+                if (!receivedToolArgumentsStreaming) {
+                    // If arguments streaming not received, just pretend we have received all the arguments
+                    // as a single JSON
+                    safeCallCallback(logger, "onToolCallRequestArgumentFragmentGenerated", baseOpts.onToolCallRequestArgumentFragmentGenerated, [predictionsPerformed, callId, JSON.stringify(request.arguments ?? {}, null, 2)]);
+                }
+                const pushedRequest = { ...request };
                 // We have now received a tool call request. Now let's see if we can call the tool and
                 // get the result.
-                toolCallRequests.push(request);
+                toolCallRequests.push(pushedRequest);
                 const tool = toolsMap.get(request.name);
                 if (tool === undefined) {
                     // Tool does not exist.
@@ -16182,15 +16406,14 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
                     toolCallPromises.push(internalHandleInvalidToolCallRequest(toolCallRequestError, request, toolCallIndex).catch(finalReject));
                     safeCallCallback(logger, "onToolCallRequestFailure", baseOpts.onToolCallRequestFailure, [
                         predictionsPerformed,
-                        currentCallId,
+                        callId,
                         toolCallRequestError,
                     ]);
                     return;
                 }
-                const parameters = request.arguments ?? {}; // Defaults to empty object
                 // Try check the parameters:
                 try {
-                    tool.checkParameters(parameters); // Defaults to empty object
+                    tool.checkParameters(pushedRequest.arguments); // Defaults to empty object
                 }
                 catch (error) {
                     // Failed to parse the parameters
@@ -16198,16 +16421,16 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
                     toolCallPromises.push(internalHandleInvalidToolCallRequest(toolCallRequestError, request, toolCallIndex).catch(finalReject));
                     safeCallCallback(logger, "onToolCallRequestFailure", baseOpts.onToolCallRequestFailure, [
                         predictionsPerformed,
-                        currentCallId,
+                        callId,
                         toolCallRequestError,
                     ]);
                     return;
                 }
-                const toolCallContext = new SimpleToolCallContext(new SimpleLogger(`Tool(${request.name})`, logger), abortController.signal, currentCallId);
+                const toolCallContext = new SimpleToolCallContext(new SimpleLogger(`Tool(${request.name})`, logger), abortController.signal, callId);
                 const isQueued = queue.needsQueueing();
                 safeCallCallback(logger, "onToolCallRequestEnd", baseOpts.onToolCallRequestEnd, [
                     predictionsPerformed,
-                    currentCallId,
+                    callId,
                     {
                         isQueued,
                         toolCallRequest: request,
@@ -16217,10 +16440,71 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
                 // We have successfully parsed the parameters. Let's call the tool.
                 toolCallPromises.push(queue
                     .runInQueue(async () => {
+                    // Emit the dequeued event if the tool call was queued.
                     if (isQueued) {
-                        safeCallCallback(logger, "onToolCallRequestDequeued", baseOpts.onToolCallRequestDequeued, [predictionsPerformed, currentCallId]);
+                        safeCallCallback(logger, "onToolCallRequestDequeued", baseOpts.onToolCallRequestDequeued, [predictionsPerformed, callId]);
+                    }
+                    // Guard the tool call if have a tool call guard.
+                    if (baseOpts.guardToolCall !== undefined) {
+                        const resultContainer = [null];
+                        await baseOpts.guardToolCall(predictionsPerformed, callId, new GuardToolCallController(request, tool, resultContainer));
+                        if (resultContainer[0] === null) {
+                            // The guard did not return anything, thus we will report this error.
+                            throw makeTitledPrettyError("Tool call guard did not allow or deny the tool call.", text `
+                      The \`guardToolCall\` handler must call one of the methods on the controller
+                      to allow or deny the tool call.
+                    `, stack);
+                        }
+                        const guardResult = resultContainer[0];
+                        const guardResultType = guardResult.type;
+                        switch (guardResultType) {
+                            case "allow": {
+                                // 1. The guard allowed the tool call without overriding the parameters. In this
+                                //    case, we will use the original parameters.
+                                break;
+                            }
+                            case "allowAndOverrideParameters": {
+                                // 2. The guard allowed the tool call and provided new parameters. In this case,
+                                //    we will use the new parameters. This will update the request in-place.
+                                pushedRequest.arguments = guardResult.parameters;
+                                break;
+                            }
+                            case "deny": {
+                                // 3. The guard denied the tool call. In this case, we will early return and not
+                                //    call the tool.
+                                toolCallResults.push({
+                                    index: toolCallIndex,
+                                    data: {
+                                        type: "toolCallResult",
+                                        toolCallId: request.id,
+                                        content: JSON.stringify({
+                                            error: guardResult.reason,
+                                        }),
+                                    },
+                                });
+                                return;
+                            }
+                        }
+                    }
+                    // Now we need to call RequestFinalized
+                    safeCallCallback(logger, "onToolCallRequestFinalized", baseOpts.onToolCallRequestFinalized, [
+                        predictionsPerformed,
+                        callId,
+                        {
+                            toolCallRequest: request,
+                            rawContent,
+                        },
+                    ]);
+                    let result;
+                    try {
+                        result = await tool.implementation(pushedRequest.arguments ?? {}, toolCallContext);
+                    }
+                    catch (error) {
+                        if (!(error instanceof UnimplementedToolError)) {
+                            throw error;
+                        }
+                        hasCalledUnimplementedTool = true;
                     }
-                    const result = await tool.implementation(parameters, toolCallContext);
                     let resultString;
                     if (result === undefined) {
                         resultString = "undefined";
@@ -16338,6 +16622,7 @@ async function internalAct(chat, tools, baseOpts, stack, logger, startTime, pred
             predictionsPerformed >= baseOpts.maxPredictionRounds) {
             shouldContinue = false;
         }
+        shouldContinue &&= !hasCalledUnimplementedTool; // Stop loop if unimplemented tool was called.
     } while (shouldContinue);
     return new ActResult(predictionsPerformed, (performance.now() - startTime) / 1_000);
 }
@@ -16612,7 +16897,7 @@ const llmActionOptsSchema = llmPredictionConfigInputSchema
     preset: z.string().optional(),
 });
 function splitActOpts(opts) {
-    const { onFirstToken, onPredictionFragment, onMessage, onRoundStart, onRoundEnd, onPredictionCompleted, onPromptProcessingProgress, onToolCallRequestStart, onToolCallRequestNameReceived, onToolCallRequestArgumentFragmentGenerated, onToolCallRequestEnd, onToolCallRequestFailure, onToolCallRequestDequeued, handleInvalidToolRequest, maxPredictionRounds, signal, preset, allowParallelToolExecution, ...config } = opts;
+    const { onFirstToken, onPredictionFragment, onMessage, onRoundStart, onRoundEnd, onPredictionCompleted, onPromptProcessingProgress, onToolCallRequestStart, onToolCallRequestNameReceived, onToolCallRequestArgumentFragmentGenerated, onToolCallRequestEnd, onToolCallRequestFinalized, onToolCallRequestFailure, onToolCallRequestDequeued, guardToolCall, handleInvalidToolRequest, maxPredictionRounds, signal, preset, allowParallelToolExecution, ...config } = opts;
     return [
         config,
         {
@@ -16627,8 +16912,10 @@ function splitActOpts(opts) {
             onToolCallRequestNameReceived,
             onToolCallRequestArgumentFragmentGenerated,
             onToolCallRequestEnd,
+            onToolCallRequestFinalized,
             onToolCallRequestFailure,
             onToolCallRequestDequeued,
+            guardToolCall,
             handleInvalidToolRequest,
             maxPredictionRounds,
             signal,
@@ -16679,6 +16966,9 @@ class LLMDynamicHandle extends DynamicHandle {
     internalPredict(history, predictionConfigStack, cancelEvent, extraOpts, onFragment, onFinished, onError) {
         let finished = false;
         let firstTokenTriggered = false;
+        let currentCallId = null;
+        let receivedEagerToolNameReporting = false;
+        let receivedToolArgumentsStreaming = false;
         const channel = this.port.createChannel("predict", {
             modelSpecifier: this.specifier,
             history,
@@ -16703,24 +16993,49 @@ class LLMDynamicHandle extends DynamicHandle {
                     break;
                 }
                 case "toolCallGenerationStart": {
-                    safeCallCallback(this.logger, "onToolCallGenerationStart", extraOpts.onToolCallRequestStart, []);
+                    if (currentCallId === null) {
+                        currentCallId = 0;
+                    }
+                    else {
+                        currentCallId++;
+                    }
+                    receivedEagerToolNameReporting = false;
+                    receivedToolArgumentsStreaming = false;
+                    safeCallCallback(this.logger, "onToolCallGenerationStart", extraOpts.onToolCallRequestStart, [currentCallId, { toolCallId: message.toolCallId }]);
                     break;
                 }
                 case "toolCallGenerationNameReceived": {
-                    safeCallCallback(this.logger, "onToolCallGenerationNameReceived", extraOpts.onToolCallRequestNameReceived, [message.name]);
+                    receivedEagerToolNameReporting = true;
+                    safeCallCallback(this.logger, "onToolCallGenerationNameReceived", extraOpts.onToolCallRequestNameReceived, [currentCallId ?? -1, message.name]);
                     break;
                 }
                 case "toolCallGenerationArgumentFragmentGenerated": {
-                    safeCallCallback(this.logger, "onToolCallGenerationArgumentFragmentGenerated", extraOpts.onToolCallRequestArgumentFragmentGenerated, [message.content]);
+                    receivedToolArgumentsStreaming = true;
+                    safeCallCallback(this.logger, "onToolCallGenerationArgumentFragmentGenerated", extraOpts.onToolCallRequestArgumentFragmentGenerated, [currentCallId ?? -1, message.content]);
                     break;
                 }
                 case "toolCallGenerationEnd": {
-                    safeCallCallback(this.logger, "onToolCallGenerationEnd", extraOpts.onToolCallRequestEnd, [{ toolCallRequest: message.toolCallRequest, rawContent: message.rawContent }]);
+                    if (!receivedEagerToolNameReporting) {
+                        // If eager name reporting not received, report it.
+                        safeCallCallback(this.logger, "onToolCallGenerationNameReceived", extraOpts.onToolCallRequestNameReceived, [currentCallId ?? -1, message.toolCallRequest.name]);
+                    }
+                    if (!receivedToolArgumentsStreaming) {
+                        // If arguments streaming not received, just pretend we have received all the
+                        // arguments as a single JSON
+                        safeCallCallback(this.logger, "onToolCallGenerationArgumentFragmentGenerated", extraOpts.onToolCallRequestArgumentFragmentGenerated, [
+                            currentCallId ?? -1,
+                            JSON.stringify(message.toolCallRequest.arguments ?? {}, null, 2),
+                        ]);
+                    }
+                    safeCallCallback(this.logger, "onToolCallGenerationEnd", extraOpts.onToolCallRequestEnd, [
+                        currentCallId ?? -1,
+                        { toolCallRequest: message.toolCallRequest, rawContent: message.rawContent },
+                    ]);
                     break;
                 }
                 case "toolCallGenerationFailed": {
                     const toolCallRequestError = new ToolCallRequestError(fromSerializedError(message.error).message, message.rawContent);
-                    safeCallCallback(this.logger, "onToolCallGenerationFailed", extraOpts.onToolCallRequestFailure, [toolCallRequestError]);
+                    safeCallCallback(this.logger, "onToolCallGenerationFailed", extraOpts.onToolCallRequestFailure, [currentCallId ?? -1, toolCallRequestError]);
                     break;
                 }
                 case "success": {
@@ -17031,7 +17346,7 @@ class LLMDynamicHandle extends DynamicHandle {
                         break;
                     }
                     case "toolCallGenerationStart": {
-                        handleToolCallGenerationStart();
+                        handleToolCallGenerationStart(message.toolCallId);
                         break;
                     }
                     case "toolCallGenerationNameReceived": {
@@ -17168,6 +17483,52 @@ class LLM extends LLMDynamicHandle {
     }
 }
 
+/** @public */
+class LLMNamespace extends ModelNamespace {
+    constructor() {
+        super(...arguments);
+        /** @internal */
+        this.namespace = "llm";
+        /** @internal */
+        this.defaultLoadConfig = {};
+        /** @internal */
+        this.loadModelConfigSchema = llmLoadModelConfigSchema;
+    }
+    /** @internal */
+    loadConfigToKVConfig(config) {
+        return llmLlamaMoeLoadConfigSchematics.buildPartialConfig({
+            "contextLength": config.contextLength,
+            "llama.evalBatchSize": config.evalBatchSize,
+            "llama.acceleration.offloadRatio": config.gpu?.ratio,
+            "load.gpuSplitConfig": convertGPUSettingToGPUSplitConfig(config.gpu),
+            "llama.flashAttention": config.flashAttention,
+            "llama.ropeFrequencyBase": numberToCheckboxNumeric(config.ropeFrequencyBase, 0, 0),
+            "llama.ropeFrequencyScale": numberToCheckboxNumeric(config.ropeFrequencyScale, 0, 0),
+            "llama.keepModelInMemory": config.keepModelInMemory,
+            "seed": numberToCheckboxNumeric(config.seed, -1, 0),
+            "llama.useFp16ForKVCache": config.useFp16ForKVCache,
+            "llama.tryMmap": config.tryMmap,
+            "numExperts": config.numExperts,
+            "llama.kCacheQuantizationType": cacheQuantizationTypeToCheckbox({
+                value: config.llamaKCacheQuantizationType,
+                falseDefault: "f16",
+            }),
+            "llama.vCacheQuantizationType": cacheQuantizationTypeToCheckbox({
+                value: config.llamaVCacheQuantizationType,
+                falseDefault: "f16",
+            }),
+        });
+    }
+    /** @internal */
+    createDomainSpecificModel(port, info, validator, logger) {
+        return new LLM(port, info, validator, logger);
+    }
+    /** @internal */
+    createDomainDynamicHandle(port, specifier, validator, logger) {
+        return new LLMDynamicHandle(port, specifier, validator, logger);
+    }
+}
+
 /**
  * Represents the result of a prediction from a generator plugin.
  *
@@ -17335,11 +17696,58 @@ class LLMGeneratorHandle {
     /** @internal */
     validator, 
     /** @internal */
+    associatedPredictionProcess, 
+    /** @internal */
     logger = new SimpleLogger(`LLMGeneratorHandle`)) {
         this.port = port;
         this.pluginIdentifier = pluginIdentifier;
         this.validator = validator;
+        this.associatedPredictionProcess = associatedPredictionProcess;
         this.logger = logger;
+        /**
+         * The identifier of the plugin that this handle is associated with.
+         */
+        this.identifier = this.pluginIdentifier;
+    }
+    getPluginConfigSpecifier(userSuppliedPluginConfig, userSuppliedWorkingDirectory, stack) {
+        if (this.associatedPredictionProcess === null) {
+            // If there is no associated prediction process, we can use the user-supplied config directly.
+            return {
+                type: "direct",
+                config: userSuppliedPluginConfig ?? emptyKVConfig,
+                workingDirectoryPath: userSuppliedWorkingDirectory ?? undefined,
+            };
+        }
+        // If there is an associated prediction process, we first need to make sure that the user has
+        // not supplied a plugin config or working directory, as these are not allowed in this case.
+        // (The plugin config/working directory of the prediction process will be used instead.)
+        if (userSuppliedPluginConfig !== undefined) {
+            throw makeTitledPrettyError("Cannot use plugin config with prediction process", text `
+          You cannot provide a plugin config to the generator handle when it is associated with a
+          prediction process. The plugin config that was configured for the prediction process will
+          be used instead.
+
+          If you want to use a different plugin config, you will need to create a separate
+          GeneratorHandle instead.
+        `, stack);
+        }
+        if (userSuppliedWorkingDirectory !== undefined) {
+            throw makeTitledPrettyError("Cannot use working directory with prediction process", text `
+          You cannot provide a working directory to the generator handle when it is associated with
+          a prediction process. The working directory that was configured for the prediction process
+          will be used instead.
+
+          If you want to use a different working directory, you will need to create a separate
+          GeneratorHandle instead.
+        `, stack);
+        }
+        // If we reach here, we can safely return the plugin config specifier for the prediction
+        // process.
+        return {
+            type: "predictionProcess",
+            pci: this.associatedPredictionProcess.pci,
+            token: this.associatedPredictionProcess.token,
+        };
     }
     /**
      * Use the generator to produce a response based on the given history.
@@ -17347,7 +17755,7 @@ class LLMGeneratorHandle {
     respond(chat, opts = {}) {
         const stack = getCurrentStack(1);
         [chat, opts] = this.validator.validateMethodParamsOrThrow("LLMGeneratorHandle", "respond", ["chat", "opts"], [chatHistoryLikeSchema, llmGeneratorPredictionOptsSchema], [chat, opts], stack);
-        const { onFirstToken, onPredictionFragment, onMessage, signal, pluginConfig = emptyKVConfig, workingDirectory, } = opts;
+        const { onFirstToken, onPredictionFragment, onMessage, signal, pluginConfig, workingDirectory, } = opts;
         let resolved = false;
         let firstTokenTriggered = false;
         const cancelEvent = new CancelEvent();
@@ -17365,9 +17773,8 @@ class LLMGeneratorHandle {
         });
         const channel = this.port.createChannel("generateWithGenerator", {
             pluginIdentifier: this.pluginIdentifier,
-            pluginConfigStack: singleLayerKVConfigStackOf("apiOverride", pluginConfig),
+            pluginConfigSpecifier: this.getPluginConfigSpecifier(pluginConfig, workingDirectory, stack),
             tools: [],
-            workingDirectoryPath: workingDirectory ?? null,
             history: accessMaybeMutableInternals(Chat.from(chat))._internalGetData(),
         }, message => {
             const messageType = message.type;
@@ -17415,7 +17822,7 @@ class LLMGeneratorHandle {
         const startTime = performance.now();
         const stack = getCurrentStack(1);
         [chat, opts] = this.validator.validateMethodParamsOrThrow("LLMGeneratorHandle", "act", ["chat", "opts"], [chatHistoryLikeSchema, llmGeneratorActOptsSchema], [chat, opts], stack);
-        const { pluginConfig = emptyKVConfig, workingDirectory = null, ...baseOpts } = opts;
+        const { pluginConfig, workingDirectory, ...baseOpts } = opts;
         const toolDefinitions = tools.map(toolToLLMTool);
         return await internalAct(chat, tools, baseOpts, stack, this.logger, startTime, 
         // Implementation of the prediction function. This performs the prediction by creating a
@@ -17424,9 +17831,8 @@ class LLMGeneratorHandle {
             // Use predict channel
             const channel = this.port.createChannel("generateWithGenerator", {
                 pluginIdentifier: this.pluginIdentifier,
-                pluginConfigStack: singleLayerKVConfigStackOf("apiOverride", pluginConfig),
+                pluginConfigSpecifier: this.getPluginConfigSpecifier(pluginConfig, workingDirectory, stack),
                 tools: allowTools ? toolDefinitions : [],
-                workingDirectoryPath: workingDirectory,
                 history,
             }, message => {
                 const messageType = message.type;
@@ -17440,7 +17846,7 @@ class LLMGeneratorHandle {
                         break;
                     }
                     case "toolCallGenerationStart": {
-                        handleToolCallGenerationStart();
+                        handleToolCallGenerationStart(message.toolCallId);
                         break;
                     }
                     case "toolCallGenerationNameReceived": {
@@ -17481,59 +17887,6 @@ class LLMGeneratorHandle {
     }
 }
 
-/** @public */
-class LLMNamespace extends ModelNamespace {
-    constructor() {
-        super(...arguments);
-        /** @internal */
-        this.namespace = "llm";
-        /** @internal */
-        this.defaultLoadConfig = {};
-        /** @internal */
-        this.loadModelConfigSchema = llmLoadModelConfigSchema;
-    }
-    /** @internal */
-    loadConfigToKVConfig(config) {
-        return llmLlamaMoeLoadConfigSchematics.buildPartialConfig({
-            "contextLength": config.contextLength,
-            "llama.evalBatchSize": config.evalBatchSize,
-            "llama.acceleration.offloadRatio": config.gpu?.ratio,
-            "load.gpuSplitConfig": convertGPUSettingToGPUSplitConfig(config.gpu),
-            "llama.flashAttention": config.flashAttention,
-            "llama.ropeFrequencyBase": numberToCheckboxNumeric(config.ropeFrequencyBase, 0, 0),
-            "llama.ropeFrequencyScale": numberToCheckboxNumeric(config.ropeFrequencyScale, 0, 0),
-            "llama.keepModelInMemory": config.keepModelInMemory,
-            "seed": numberToCheckboxNumeric(config.seed, -1, 0),
-            "llama.useFp16ForKVCache": config.useFp16ForKVCache,
-            "llama.tryMmap": config.tryMmap,
-            "numExperts": config.numExperts,
-            "llama.kCacheQuantizationType": cacheQuantizationTypeToCheckbox({
-                value: config.llamaKCacheQuantizationType,
-                falseDefault: "f16",
-            }),
-            "llama.vCacheQuantizationType": cacheQuantizationTypeToCheckbox({
-                value: config.llamaVCacheQuantizationType,
-                falseDefault: "f16",
-            }),
-        });
-    }
-    /** @internal */
-    createDomainSpecificModel(port, info, validator, logger) {
-        return new LLM(port, info, validator, logger);
-    }
-    /** @internal */
-    createDomainDynamicHandle(port, specifier, validator, logger) {
-        return new LLMDynamicHandle(port, specifier, validator, logger);
-    }
-    /**
-     * @experimental [EXP-GEN-PREDICT] Using generator plugins programmatically is still in development.
-     * This may change in the future without warning.
-     */
-    createGeneratorHandle(pluginIdentifier) {
-        return new LLMGeneratorHandle(this.port, pluginIdentifier, this.validator, this.logger);
-    }
-}
-
 const generatorSchema = z.function();
 
 /**
@@ -17689,8 +18042,8 @@ class GeneratorController extends BaseController {
      * successfully generated tool calls, or a `toolCallGenerationFailed` call for
      * failed tool calls.
      */
-    toolCallGenerationStarted() {
-        this.connector.toolCallGenerationStarted();
+    toolCallGenerationStarted({ toolCallId, } = {}) {
+        this.connector.toolCallGenerationStarted(toolCallId);
     }
     /**
      * Use this function to report that the name of the tool call has been generated. This function
@@ -17736,7 +18089,7 @@ class GeneratorController extends BaseController {
     }
 }
 
-var __addDisposableResource = (globalThis && globalThis.__addDisposableResource) || function (env, value, async) {
+var __addDisposableResource$1 = (globalThis && globalThis.__addDisposableResource) || function (env, value, async) {
     if (value !== null && value !== void 0) {
         if (typeof value !== "object" && typeof value !== "function") throw new TypeError("Object expected.");
         var dispose, inner;
@@ -17758,7 +18111,7 @@ var __addDisposableResource = (globalThis && globalThis.__addDisposableResource)
     }
     return value;
 };
-var __disposeResources = (globalThis && globalThis.__disposeResources) || (function (SuppressedError) {
+var __disposeResources$1 = (globalThis && globalThis.__disposeResources) || (function (SuppressedError) {
     return function (env) {
         function fail(e) {
             env.error = env.hasError ? new SuppressedError(e, env.error, "An error was suppressed during disposal.") : e;
@@ -17859,12 +18212,12 @@ class ProcessingConnector {
         // argument.
         return Chat.createRaw(chatHistoryData, /* mutable */ false).asMutableCopy();
     }
-    async getOrLoadModel() {
-        const result = await this.pluginsPort.callRpc("processingGetOrLoadModel", {
+    async getOrLoadTokenSource() {
+        const result = await this.pluginsPort.callRpc("processingGetOrLoadTokenSource", {
             pci: this.processingContextIdentifier,
             token: this.token,
         });
-        return result.identifier;
+        return result.tokenSourceIdentifier;
     }
     async hasStatus() {
         return await this.pluginsPort.callRpc("processingHasStatus", {
@@ -17891,7 +18244,7 @@ class ProcessingConnector {
  */
 class ProcessingController extends BaseController {
     /** @internal */
-    constructor(client, pluginConfig, globalPluginConfig, workingDirectoryPath, 
+    constructor(client, pluginConfig, globalPluginConfig, workingDirectoryPath, enabledPluginInfos, 
     /** @internal */
     connector, 
     /** @internal */
@@ -17903,27 +18256,10 @@ class ProcessingController extends BaseController {
      */
     shouldIncludeCurrentInHistory) {
         super(client, connector.abortSignal, pluginConfig, globalPluginConfig, workingDirectoryPath);
+        this.enabledPluginInfos = enabledPluginInfos;
         this.connector = connector;
         this.config = config;
         this.shouldIncludeCurrentInHistory = shouldIncludeCurrentInHistory;
-        this.model = Object.freeze({
-            getOrLoad: async () => {
-                const identifier = await this.connector.getOrLoadModel();
-                const model = await this.client.llm.model(identifier);
-                // Don't use the server session config for this model
-                model.internalIgnoreServerSessionConfig = true;
-                // Inject the prediction config
-                model.internalKVConfigStack = {
-                    layers: [
-                        {
-                            layerName: "conversationSpecific",
-                            config: this.config,
-                        },
-                    ],
-                };
-                return model;
-            },
-        });
         this.processingControllerHandle = {
             abortSignal: connector.abortSignal,
             sendUpdate: update => {
@@ -18023,8 +18359,40 @@ class ProcessingController extends BaseController {
     debug(...messages) {
         this.createDebugInfoBlock(concatenateDebugMessages(...messages));
     }
-    getPredictionConfig() {
-        return kvConfigToLLMPredictionConfig(this.config);
+    /**
+     * Gets the token source associated with this prediction process (i.e. what the user has selected
+     * on the top navigation bar).
+     *
+     * The token source can either be a model or a generator plugin. In both cases, the returned
+     * object will contain a ".act" and a ".respond" method, which can be used to generate text.
+     *
+     * The token source is already pre-configured to use user's prediction config - you don't need to
+     * pass through any additional configuration.
+     */
+    async tokenSource() {
+        const tokenSourceIdentifier = await this.connector.getOrLoadTokenSource();
+        const tokenSourceIdentifierType = tokenSourceIdentifier.type;
+        switch (tokenSourceIdentifierType) {
+            case "model": {
+                const model = await this.client.llm.model(tokenSourceIdentifier.identifier);
+                // Don't use the server session config for this model
+                model.internalIgnoreServerSessionConfig = true;
+                // Inject the prediction config
+                model.internalKVConfigStack = {
+                    layers: [
+                        {
+                            layerName: "conversationSpecific",
+                            config: this.config,
+                        },
+                    ],
+                };
+                return model;
+            }
+            case "generator": {
+                const generator = this.client.plugins.createGeneratorHandleAssociatedWithPredictionProcess(tokenSourceIdentifier.pluginIdentifier, this.connector.processingContextIdentifier, this.connector.token);
+                return generator;
+            }
+        }
     }
     /**
      * Sets the sender name for this message. The sender name shown above the message in the chat.
@@ -18103,6 +18471,31 @@ class ProcessingController extends BaseController {
         const toolStatusController = new PredictionProcessToolStatusController(this.processingControllerHandle, id, initialStatus);
         return toolStatusController;
     }
+    /**
+     * Starts a tool use session with tools available in the prediction process. Note, this method
+     * should be used with "Explicit Resource Management". That is, you should use it like so:
+     *
+     * ```typescript
+     * using toolUseSession = await ctl.startToolUseSession();
+     * // ^ Notice the `using` keyword here.
+     * ```
+     *
+     * If you do not `using`, you should call `toolUseSession[Symbol.dispose]()` after you are done.
+     *
+     * If you don't, lmstudio-js will close the session upon the end of the prediction step
+     * automatically. However, it is not recommended.
+     *
+     * @public
+     * @deprecated WIP
+     */
+    async startToolUseSession() {
+        const identifiersOfPluginsWithTools = this.enabledPluginInfos
+            .filter(({ hasToolsProvider }) => hasToolsProvider)
+            .map(({ identifier }) => identifier);
+        return await this.client.plugins.startToolUseSessionUsingPredictionProcess(
+        // We start a tool use session with all the plugins that have tools available
+        identifiersOfPluginsWithTools, this.connector.processingContextIdentifier, this.connector.token);
+    }
 }
 /**
  * Controller for a status block in the prediction process.
@@ -18301,7 +18694,7 @@ class PredictionProcessContentBlockController {
     async pipeFrom(prediction) {
         const env_1 = { stack: [], error: void 0, hasError: false };
         try {
-            const cleaner = __addDisposableResource(env_1, new Cleaner(), false);
+            const cleaner = __addDisposableResource$1(env_1, new Cleaner(), false);
             const abortListener = () => {
                 prediction.cancel();
             };
@@ -18328,7 +18721,7 @@ class PredictionProcessContentBlockController {
             env_1.hasError = true;
         }
         finally {
-            __disposeResources(env_1);
+            __disposeResources$1(env_1);
         }
     }
 }
@@ -18409,10 +18802,11 @@ class GeneratorConnectorImpl {
             opts: opts,
         });
     }
-    toolCallGenerationStarted() {
+    toolCallGenerationStarted(toolCallId) {
         this.channel.send({
             type: "toolCallGenerationStarted",
             taskId: this.taskId,
+            toolCallId,
         });
     }
     toolCallGenerationNameReceived(toolName) {
@@ -18475,7 +18869,7 @@ class PluginSelfRegistrationHost {
                     const abortController = new AbortController();
                     const connector = new ProcessingConnector(this.port, abortController.signal, message.pci, message.token, taskLogger);
                     const input = ChatMessage.createRaw(message.input, /* mutable */ false);
-                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, connector, message.config, 
+                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, message.enabledPluginInfos, connector, message.config, 
                     /* shouldIncludeInputInHistory */ false);
                     tasks.set(message.taskId, {
                         cancel: () => {
@@ -18545,7 +18939,7 @@ class PluginSelfRegistrationHost {
         }, { stack });
     }
     /**
-     * Sets the promptPreprocessor to be used by the plugin represented by this client.
+     * Sets the prediction loop handler to be used by the plugin represented by this client.
      *
      * @deprecated [DEP-PLUGIN-PREDICTION-LOOP-HANDLER] Prediction loop handler support is still in
      * development. Stay tuned for updates.
@@ -18563,7 +18957,7 @@ class PluginSelfRegistrationHost {
                     taskLogger.info(`New prediction loop handling request received.`);
                     const abortController = new AbortController();
                     const connector = new ProcessingConnector(this.port, abortController.signal, message.pci, message.token, taskLogger);
-                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, connector, message.config, 
+                    const controller = new ProcessingController(this.client, message.pluginConfig, message.globalPluginConfig, message.workingDirectoryPath, message.enabledPluginInfos, connector, message.config, 
                     /* shouldIncludeInputInHistory */ true);
                     tasks.set(message.taskId, {
                         cancel: () => {
@@ -18942,6 +19336,309 @@ class PluginSelfRegistrationHost {
     }
 }
 
+var __addDisposableResource = (globalThis && globalThis.__addDisposableResource) || function (env, value, async) {
+    if (value !== null && value !== void 0) {
+        if (typeof value !== "object" && typeof value !== "function") throw new TypeError("Object expected.");
+        var dispose, inner;
+        if (async) {
+            if (!Symbol.asyncDispose) throw new TypeError("Symbol.asyncDispose is not defined.");
+            dispose = value[Symbol.asyncDispose];
+        }
+        if (dispose === void 0) {
+            if (!Symbol.dispose) throw new TypeError("Symbol.dispose is not defined.");
+            dispose = value[Symbol.dispose];
+            if (async) inner = dispose;
+        }
+        if (typeof dispose !== "function") throw new TypeError("Object not disposable.");
+        if (inner) dispose = function() { try { inner.call(this); } catch (e) { return Promise.reject(e); } };
+        env.stack.push({ value: value, dispose: dispose, async: async });
+    }
+    else if (async) {
+        env.stack.push({ async: true });
+    }
+    return value;
+};
+var __disposeResources = (globalThis && globalThis.__disposeResources) || (function (SuppressedError) {
+    return function (env) {
+        function fail(e) {
+            env.error = env.hasError ? new SuppressedError(e, env.error, "An error was suppressed during disposal.") : e;
+            env.hasError = true;
+        }
+        var r, s = 0;
+        function next() {
+            while (r = env.stack.pop()) {
+                try {
+                    if (!r.async && s === 1) return s = 0, env.stack.push(r), Promise.resolve().then(next);
+                    if (r.dispose) {
+                        var result = r.dispose.call(r.value);
+                        if (r.async) return s |= 2, Promise.resolve(result).then(next, function(e) { fail(e); return next(); });
+                    }
+                    else s |= 1;
+                }
+                catch (e) {
+                    fail(e);
+                }
+            }
+            if (s === 1) return env.hasError ? Promise.reject(env.error) : Promise.resolve();
+            if (env.hasError) throw env.error;
+        }
+        return next();
+    };
+})(typeof SuppressedError === "function" ? SuppressedError : function (error, suppressed, message) {
+    var e = new Error(message);
+    return e.name = "SuppressedError", e.error = error, e.suppressed = suppressed, e;
+});
+/**
+ * Represents a tool use session backed by a single plugin. Don't construct this class yourself.
+ *
+ * @public
+ */
+class SingleRemoteToolUseSession {
+    static async create(pluginsPort, pluginIdentifier, pluginConfigSpecifier, logger, stack) {
+        const session = new SingleRemoteToolUseSession(pluginsPort, pluginIdentifier, pluginConfigSpecifier, logger);
+        await session.init(stack);
+        return session;
+    }
+    constructor(pluginsPort, pluginIdentifier, pluginConfigSpecifier, logger) {
+        this.pluginsPort = pluginsPort;
+        this.pluginIdentifier = pluginIdentifier;
+        this.pluginConfigSpecifier = pluginConfigSpecifier;
+        this.logger = logger;
+        this.status = "initializing";
+        /**
+         * Whether this session is "poisoned". A session is poisoned either when the underlying channel
+         * has errored/closed.
+         */
+        this.poison = null;
+        /**
+         * Map to track all the ongoing tool calls.
+         */
+        this.ongoingToolCalls = new Map();
+        this.callIdGiver = new IdGiver(0);
+    }
+    async init(stack) {
+        const { promise: initPromise, resolve: resolveInit, reject: rejectInit } = makePromise();
+        const channel = this.pluginsPort.createChannel("startToolUseSession", {
+            pluginIdentifier: this.pluginIdentifier,
+            pluginConfigSpecifier: this.pluginConfigSpecifier,
+        }, message => {
+            const messageType = message.type;
+            switch (messageType) {
+                // Upon receiving session ready, mark self as ready and resolve the promise.
+                case "sessionReady": {
+                    if (this.status !== "initializing") {
+                        this.logger.error("Received sessionReady message while not initializing");
+                        return;
+                    }
+                    this.status = "ready";
+                    resolveInit();
+                    this.tools = message.toolDefinitions.map(toolDefinition => this.makeTool(toolDefinition));
+                    break;
+                }
+                case "toolCallComplete": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.resolve(message.result);
+                    break;
+                }
+                case "toolCallError": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.reject(fromSerializedError(message.error));
+                    break;
+                }
+                case "toolCallStatus": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.reportStatus(message.statusText);
+                    break;
+                }
+                case "toolCallWarn": {
+                    const ongoingCall = this.ongoingToolCalls.get(message.callId);
+                    if (ongoingCall === undefined) {
+                        return;
+                    }
+                    ongoingCall.reportWarning(message.warnText);
+                    break;
+                }
+                default: {
+                    const exhaustiveCheck = messageType;
+                    this.logger.warn(`Received unexpected message type in tool use session: ${exhaustiveCheck}`);
+                }
+            }
+        }, { stack });
+        channel.onError.subscribeOnce(error => {
+            if (this.status === "initializing") {
+                // If still initializing, reject the promise with the error.
+                rejectInit(error);
+            }
+            else {
+                this.logger.error("Tool use session error.", error);
+                this.poison = error;
+            }
+            // Reject all ongoing tool calls with the error.
+            for (const ongoingCall of this.ongoingToolCalls.values()) {
+                ongoingCall.reject(error);
+            }
+            this.status = "disposed";
+        });
+        channel.onClose.subscribeOnce(() => {
+            let error;
+            if (this.status === "initializing") {
+                // If still initializing, reject the promise with the error.
+                error = new Error("Tool use session channel closed unexpectedly during initialization.");
+                rejectInit(error);
+            }
+            else {
+                error = new Error("Tool use session has already ended.");
+                // We don't print an error here because channel can close normally. We only poison this
+                // session so it throws the error when used.
+                this.poison = error;
+            }
+            // Reject all ongoing tool calls with the error.
+            for (const ongoingCall of this.ongoingToolCalls.values()) {
+                ongoingCall.reject(error);
+            }
+            this.status = "disposed";
+        });
+        this.channel = channel;
+        await initPromise;
+    }
+    [Symbol.dispose]() {
+        // As long as we are not already disposed, we send a discard message to the channel.
+        if (this.status !== "disposed") {
+            this.channel.send({ type: "discardSession" });
+            this.status = "disposed";
+            const error = new Error("Session disposed by client.");
+            // Reject all ongoing tool calls with the error.
+            for (const ongoingCall of this.ongoingToolCalls.values()) {
+                ongoingCall.reject(error);
+            }
+            this.poison = error;
+        }
+    }
+    makeTool(toolDefinition) {
+        return internalCreateRemoteTool({
+            name: toolDefinition.function.name,
+            description: toolDefinition.function.description ?? "",
+            pluginIdentifier: this.pluginIdentifier,
+            parametersJsonSchema: toolDefinition.function.parameters ?? {},
+            implementation: async (args, ctx) => {
+                const env_1 = { stack: [], error: void 0, hasError: false };
+                try {
+                    // We now need to provide an implementation that basically proxies the execution of the tool
+                    // to the backend.
+                    if (this.poison !== null) {
+                        // If the session is already poisoned, throw the error.
+                        throw this.poison;
+                    }
+                    // Handling the case where the request is already aborted before we start the tool call.
+                    if (ctx.signal.aborted) {
+                        throw ctx.signal.reason;
+                    }
+                    const callId = this.callIdGiver.next();
+                    const { promise, resolve, reject } = makePromise();
+                    const cleaner = __addDisposableResource(env_1, new Cleaner(), false);
+                    this.ongoingToolCalls.set(callId, {
+                        callId,
+                        resolve,
+                        reject,
+                        reportStatus: status => ctx.status(status),
+                        reportWarning: warning => ctx.warn(warning),
+                    });
+                    cleaner.register(() => {
+                        this.ongoingToolCalls.delete(callId);
+                    });
+                    this.channel.send({
+                        type: "callTool",
+                        callId,
+                        name: toolDefinition.function.name,
+                        arguments: args,
+                    });
+                    ctx.signal.addEventListener("abort", () => {
+                        if (this.status === "disposed") {
+                            return;
+                        }
+                        this.channel.send({
+                            type: "abortToolCall",
+                            callId,
+                        });
+                        reject(ctx.signal.reason);
+                    }, { once: true });
+                    return await promise;
+                }
+                catch (e_1) {
+                    env_1.error = e_1;
+                    env_1.hasError = true;
+                }
+                finally {
+                    __disposeResources(env_1);
+                }
+            },
+        });
+    }
+}
+/**
+ * Represents a tool use session backed by multiple plugins. Don't construct this class yourself.
+ *
+ * @public
+ */
+class MultiRemoteToolUseSession {
+    static async createUsingPredictionProcess(pluginsPort, pluginIdentifiers, predictionContextIdentifier, token, logger, stack) {
+        // Start initializing all the sessions in parallel. This is OK because usually all the plugins
+        // are already loaded for the prediction process anyway.
+        const results = await Promise.allSettled(pluginIdentifiers.map(pluginIdentifier => SingleRemoteToolUseSession.create(pluginsPort, pluginIdentifier, {
+            type: "predictionProcess",
+            pci: predictionContextIdentifier,
+            token,
+        }, logger, stack)));
+        const failed = results.filter(result => result.status === "rejected");
+        if (failed.length > 0) {
+            // Some sessions failed to initialize. We need to terminate all the sessions that
+            // successfully initialized.
+            for (const result of results) {
+                if (result.status === "fulfilled") {
+                    try {
+                        result.value[Symbol.dispose]();
+                    }
+                    catch (error) {
+                        logger.error("Failed to dispose a session after initialization failure.", error);
+                    }
+                }
+            }
+            throw new AggregateError(failed.map(result => result.reason), "Failed to initialize some tool use sessions.");
+        }
+        return new MultiRemoteToolUseSession(results.map(result => result.value), logger);
+    }
+    constructor(sessions, logger) {
+        this.sessions = sessions;
+        this.logger = logger;
+        this.tools = [];
+        this.tools = sessions.flatMap(session => session.tools);
+    }
+    [Symbol.dispose]() {
+        // Dispose all the sessions.
+        for (const session of this.sessions) {
+            try {
+                session[Symbol.dispose]();
+            }
+            catch (error) {
+                this.logger.error("Failed to dispose a session.", error);
+            }
+        }
+    }
+}
+
+const pluginToolsOptsSchema = z.object({
+    pluginConfig: kvConfigSchema.optional(),
+    workingDirectory: z.string().optional(),
+});
 const registerDevelopmentPluginOptsSchema = z.object({
     manifest: pluginManifestSchema,
 });
@@ -19016,6 +19713,66 @@ class PluginsNamespace {
     getSelfRegistrationHost() {
         return new PluginSelfRegistrationHost(this.port, this.client, this.rootLogger, this.validator);
     }
+    /**
+     * Starts a tool use session use any config specifier.
+     */
+    async internalStartToolUseSession(pluginIdentifier, pluginConfigSpecifier, _stack) {
+        return await SingleRemoteToolUseSession.create(this.port, pluginIdentifier, pluginConfigSpecifier, this.logger);
+    }
+    /**
+     * Start a tool use session with a plugin. Note, this method must be used with "Explicit Resource
+     * Management". That is, you should use it like so:
+     *
+     * ```typescript
+     * using pluginTools = await client.plugins.pluginTools("owner/name", { ... });
+     * // ^ Notice the `using` keyword here.
+     * ```
+     *
+     * If you do not use `using`, you must call `pluginTools[Symbol.dispose]()` after you are done.
+     * Otherwise, there will be a memory leak and the plugins you requested tools from will be loaded
+     * indefinitely.
+     *
+     * @experimental [EXP-USE-USE-PLUGIN-TOOLS] Using tools from other applications is still in
+     * development. This may change in the future without warning.
+     */
+    async pluginTools(pluginIdentifier, opts = {}) {
+        const stack = getCurrentStack(1);
+        [pluginIdentifier, opts] = this.validator.validateMethodParamsOrThrow("plugins", "pluginTools", ["pluginIdentifier", "opts"], [artifactIdentifierSchema, pluginToolsOptsSchema], [pluginIdentifier, opts], stack);
+        return await this.internalStartToolUseSession(pluginIdentifier, {
+            type: "direct",
+            config: opts.pluginConfig ?? emptyKVConfig,
+            workingDirectoryPath: opts.workingDirectory,
+        });
+    }
+    /**
+     * Start a tool use session associated with a prediction process.
+     *
+     * This method is used internally by processing controllers and will be stripped by the internal
+     * tag.
+     *
+     * @internal
+     */
+    async startToolUseSessionUsingPredictionProcess(pluginIdentifiers, predictionContextIdentifier, token, stack) {
+        return await MultiRemoteToolUseSession.createUsingPredictionProcess(this.port, pluginIdentifiers, predictionContextIdentifier, token, this.logger, stack);
+    }
+    /**
+     * @experimental [EXP-GEN-PREDICT] Using generator plugins programmatically is still in
+     * development. This may change in the future without warning.
+     */
+    createGeneratorHandle(pluginIdentifier) {
+        return new LLMGeneratorHandle(this.port, pluginIdentifier, this.validator, null, this.logger);
+    }
+    /**
+     * Creates a generator handle that is already associated with a prediction process.
+     *
+     * This method is used internally by the processing controllers to create generator handles. It is
+     * marked as internal and will be stripped.
+     *
+     * @internal
+     */
+    createGeneratorHandleAssociatedWithPredictionProcess(pluginIdentifier, predictionContextIdentifier, token) {
+        return new LLMGeneratorHandle(this.port, pluginIdentifier, this.validator, { pci: predictionContextIdentifier, token }, this.logger);
+    }
 }
 
 const artifactDownloadPlannerDownloadOptsSchema = z.object({
@@ -19637,7 +20394,7 @@ class LMStudioClient {
         }
         // On browser, those apiServerPorts are not accessible anyway. We will just try to see if we can
         // reach the server on 127.0.0.1:1234 (the default port).
-        if (process$1.browser) {
+        if (process.browser) {
             try {
                 this.isLocalhostWithGivenPortLMStudioServer(1234);
                 return "ws://127.0.0.1:1234";
@@ -19767,4 +20524,4 @@ class LMStudioClient {
     }
 }
 
-export { Chat, ChatMessage, FileHandle, LMStudioClient, MaybeMutable, ToolCallRequestError, createConfigSchematics, kvValueTypesLibrary, rawFunctionTool, text, tool };
+export { Chat, ChatMessage, FileHandle, LMStudioClient, MaybeMutable, ToolCallRequestError, createConfigSchematics, kvValueTypesLibrary, rawFunctionTool, text, tool, unimplementedRawFunctionTool };
diff --git a/node_modules/@lmstudio/sdk/dist/tsdoc-metadata.json b/node_modules/@lmstudio/sdk/dist/tsdoc-metadata.json
new file mode 100644
index 0000000..f75cd3f
--- /dev/null
+++ b/node_modules/@lmstudio/sdk/dist/tsdoc-metadata.json
@@ -0,0 +1,11 @@
+// This file is read by tools that parse documentation comments conforming to the TSDoc standard.
+// It should be published with your NPM package.  It should not be tracked by Git.
+{
+  "tsdocVersion": "0.12",
+  "toolPackages": [
+    {
+      "packageName": "@microsoft/api-extractor",
+      "packageVersion": "7.40.1"
+    }
+  ]
+}
