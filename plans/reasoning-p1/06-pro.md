Below is a mechanicsâ€‘only checklist to bring multiâ€‘llmâ€‘ts over to the new Responsesâ€¯API.
Each step is ordered so that you can run the unitâ€‘tests after every change and keep the delta small.
Lines marked â€œğŸ‘‰ commit pointâ€ are safe places to snapshot.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
	1.	Upgrade and preliminaries
1.1  npm i openai@^5 â€‘ earliest version with client.responses.*.
1.2  Add export OPENAI_API_VERSION=2025â€‘04â€‘15 (latest GA date).
1.3  Ensure the current defaultBaseUrl (https://api.openai.com/v1) stays unchanged â€“ the SDK selects the correct subâ€‘path internally.
ğŸ‘‰ commit point: SDK upgrade & typeâ€‘checks green.
	2.	Gateâ€‘switch in openai.ts
2.1  Keep the helper you already wrote:

    export function modelSupportsResponses(modelId: string) {
      return modelId.startsWith('o3') || modelId.startsWith('o4');
    }

2.2  In constructor, honour config.preferResponses and the helper above:

    this.useResponses =
       config.preferResponses && modelSupportsResponses(config.model);

ğŸ‘‰ commit point

	3.	Requestâ€‘builder helpers
3.1  Add two private helpers:

â€¢ `buildChatCompletionsRequest(thread: Message[], opts: â€¦)`  
â€¢ `buildResponsesRequest(thread: Message[], opts: â€¦)`

The second one maps the *last* user message to `input`, merges every
system message into a single `instructions` string, andâ€”if you have
already had at least one turnâ€”sets `previous_response_id` from
`threadCtx.lastResponseId`.

(Reason: Responses API is **stateful**; you never resend the entire
history.  See example with `previous_response_id` in Zack Saadiouiâ€™s
blog postÂ  [oai_citation:0â€¡arsturn.com](https://www.arsturn.com/blog/demystifying-the-workings-of-the-responses-api-for-developers).)

ğŸ‘‰ commit point

	4.	Callâ€‘site switch
4.1  Replace the single this.client.chat.completions.create(...)
with:

    if (this.useResponses) {
      return this.client.responses.create(await this.buildResponsesRequest(...));
    } else {
      return this.client.chat.completions.create(await this.buildChatCompletionsRequest(...));
    }

 Pass `stream: true` exactly the same way for both paths.

ğŸ‘‰ commit point

	5.	Streaming parser
5.1  The Responses stream emits typed events, not a single delta
object.  Handle at least:

 â€¢ `outputTextDelta`  â†’ yield `{ type: 'text', delta: e.delta }`  
 â€¢ `functionCallArgumentsDelta` â†’ accumulate args  
 â€¢ `toolCallCreated` / `toolCallFailed` / `toolCallDone` â†’ map to your existing `LlmToolCall*` events  
 â€¢ `responseCompleted` â†’ remember `response.id` in `threadCtx.lastResponseId`

 The event names are listed in JamesÂ Rochabrunâ€™s post (see bullet list of
 â€œKey Features Supportedâ€) Â  [oai_citation:1â€¡jamesrochabrun.medium.com](https://jamesrochabrun.medium.com/stream-real-time-ai-responses-in-swift-with-openais-response-api-ad599e532f95).

5.2  Do not expect a choices array; every event is already for a
single response.
ğŸ‘‰ commit point with an interim unitâ€‘test that feeds a recorded SSE log.

	6.	Usage accounting
6.1  The Responses payload returns

```json
"usage": {
  "input_tokens": 37,
  "output_tokens": 125,
  "reasoning_tokens": 14
}
```

 Extend `parseUsage()` to read those keys as fallâ€‘back when
 `prompt_tokens` / `completion_tokens` are absent.  (Early adopters hit
 this exact errorÂ  [oai_citation:2â€¡github.com](https://github.com/zed-industries/zed/discussions/32550).)

ğŸ‘‰ commit point

	7.	Tool definitions
7.1  When you build the request, transform internal tool specs to the API
shape:

    tools: [{ type: 'function', function: { name, description, parameters } }]

 or for hosted tools, e.g.:

    tools: [{ type: 'web_search_preview' }]

 The API will decide when to call; no functionâ€‘call schema header is
 needed for builtâ€‘insÂ  [oai_citation:3â€¡arsturn.com](https://www.arsturn.com/blog/demystifying-the-workings-of-the-responses-api-for-developers).

ğŸ‘‰ commit point

	8.	Continue/Fork helpers (optional but recommended)
8.1  Add thin wrappers:

â€¢ `continueResponse(previousId: string, input: string, opts?: â€¦)`  
â€¢ `forkResponse(previousId: string, input: string, opts?: â€¦)`

 These simply call `client.responses.create` with
 `previous_response_id` set.  They let higherâ€‘level code choose whether
 to branch the conversation or keep it linear.

ğŸ‘‰ commit point

	9.	Regression tests
9.1  Add happyâ€‘path and toolâ€‘call streaming fixtures recorded with nock.
9.2  Assert that you never send more than one user message per call when
this.useResponses is true â€“ bulletÂ 1 in the GitHub discussionÂ  ï¿¼.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key behavioural differences to remember
	â€¢	The API is stateful â€“ you supply only the new user input plus, optionally, previous_response_id; context is stored serverâ€‘side.
	â€¢	Events are typed SSE chunks; there is no outer choices array.
	â€¢	Builtâ€‘in tools (web_search, file_search, code_interpreter) are firstâ€‘class; treat them exactly as you treat custom functions.
	â€¢	Token accounting fields differ (input_tokens, output_tokens, reasoning_tokens).
	â€¢	You can resume or branch a conversation with .create({â€¦, previous_response_id}) instead of reâ€‘sending the full thread.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Quick selfâ€‘check before closing the branch

â˜‘ Unitâ€‘tests green with and without preferResponses.
â˜‘ Last response ID is persisted and used on the next turn.
â˜‘ Streaming still yields LlmChunk objects in the same shape as before.
â˜‘ Usage numbers are nonâ€‘zero again; no exceptions thrown for missing keys.
â˜‘ Network recorder shows one POST per turn (no full history payloads).

Follow the commit points and you can swap the transport in one afternoon without destabilising the rest of multiâ€‘llmâ€‘ts.